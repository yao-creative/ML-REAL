this is a list of numerical analysis topics.


== general ==
validated numerics
iterative method
rate of convergence — the speed at which a convergent sequence approaches its limit
order of accuracy — rate at which numerical solution of differential equation converges to exact solution
series acceleration — methods to accelerate the speed of convergence of a series
aitken's delta-squared process — most useful for linearly converging sequences
minimum polynomial extrapolation — for vector sequences
richardson extrapolation
shanks transformation — similar to aitken's delta-squared process, but applied to the partial sums
van wijngaarden transformation — for accelerating the convergence of an alternating series
abramowitz and stegun — book containing formulas and tables of many special functions
digital library of mathematical functions — successor of book by abramowitz and stegun
curse of dimensionality
local convergence and global convergence — whether you need a good initial guess to get convergence
superconvergence
discretization
difference quotient
complexity:
computational complexity of mathematical operations
smoothed analysis — measuring the expected performance of algorithms under slight random perturbations of worst-case inputs
symbolic-numeric computation — combination of symbolic and numeric methods
cultural and historical aspects:
history of numerical solution of differential equations using computers
hundred-dollar, hundred-digit challenge problems — list of ten problems proposed by nick trefethen in 2002
international workshops on lattice qcd and numerical analysis
timeline of numerical analysis after 1945
general classes of methods:
collocation method — discretizes a continuous equation by requiring it only to hold at certain points
level-set method
level set (data structures) — data structures for representing level sets
sinc numerical methods — methods based on the sinc function, sinc(x) = sin(x) / x
abs methods


== error ==
error analysis (mathematics)

approximation
approximation error
catastrophic cancellation
condition number
discretization error
floating point number
guard digit — extra precision introduced during a computation to reduce round-off error
truncation — rounding a floating-point number by discarding all digits after a certain digit
round-off error
numeric precision in microsoft excel
arbitrary-precision arithmetic
interval arithmetic — represent every number by two floating-point numbers guaranteed to have the unknown number between them
interval contractor — maps interval to subinterval which still contains the unknown exact answer
interval propagation — contracting interval domains without removing any value consistent with the constraints
see also: interval boundary element method, interval finite element
loss of significance
numerical error
numerical stability
error propagation:
propagation of uncertainty
list of uncertainty propagation software
significance arithmetic
residual (numerical analysis)
relative change and difference — the relative difference between x and y is |x − y| / max(|x|, |y|)
significant figures
false precision — giving more significant figures than appropriate
sterbenz lemma
truncation error — error committed by doing only a finite numbers of steps
well-posed problem
affine arithmetic


== elementary and special functions ==
unrestricted algorithm
summation:
kahan summation algorithm
pairwise summation — slightly worse than kahan summation but cheaper
binary splitting
2sum
multiplication:
multiplication algorithm — general discussion, simple methods
karatsuba algorithm — the first algorithm which is faster than straightforward multiplication
toom–cook multiplication — generalization of karatsuba multiplication
schönhage–strassen algorithm — based on fourier transform, asymptotically very fast
fürer's algorithm — asymptotically slightly faster than schönhage–strassen
division algorithm — for computing quotient and/or remainder of two numbers
long division
restoring division
non-restoring division
srt division
newton–raphson division: uses newton's method to find the reciprocal of d, and multiply that reciprocal by n to find the final quotient q.
goldschmidt division
exponentiation:
exponentiation by squaring
addition-chain exponentiation
multiplicative inverse algorithms: for computing a number's multiplicative inverse (reciprocal).
newton's method
polynomials:
horner's method
estrin's scheme — modification of the horner scheme with more possibilities for parallelization
clenshaw algorithm
de casteljau's algorithm
square roots and other roots:
integer square root
methods of computing square roots
nth root algorithm
shifting nth root algorithm — similar to long division
hypot — the function (x2 + y2)1/2
alpha max plus beta min algorithm — approximates hypot(x,y)
fast inverse square root — calculates 1 / √x using details of the ieee floating-point system
elementary functions (exponential, logarithm, trigonometric functions):
trigonometric tables — different methods for generating them
cordic — shift-and-add algorithm using a table of arc tangents
bkm algorithm — shift-and-add algorithm using a table of logarithms and complex numbers
gamma function:
lanczos approximation
spouge's approximation — modification of stirling's approximation; easier to apply than lanczos
agm method — computes arithmetic–geometric mean; related methods compute special functions
fee method (fast e-function evaluation) — fast summation of series like the power series for ex
gal's accurate tables — table of function values with unequal spacing to reduce round-off error
spigot algorithm — algorithms that can compute individual digits of a real number
approximations of π:
liu hui's π algorithm — first algorithm that can compute π to arbitrary precision
leibniz formula for π — alternating series with very slow convergence
wallis product — infinite product converging slowly to π/2
viète's formula — more complicated infinite product which converges faster
gauss–legendre algorithm — iteration which converges quadratically to π, based on arithmetic–geometric mean
borwein's algorithm — iteration which converges quartically to 1/π, and other algorithms
chudnovsky algorithm — fast algorithm that calculates a hypergeometric series
bailey–borwein–plouffe formula — can be used to compute individual hexadecimal digits of π
bellard's formula — faster version of bailey–borwein–plouffe formula
list of formulae involving π


== numerical linear algebra ==
numerical linear algebra — study of numerical algorithms for linear algebra problems


=== basic concepts ===
types of matrices appearing in numerical analysis:
sparse matrix
band matrix
bidiagonal matrix
tridiagonal matrix
pentadiagonal matrix
skyline matrix
circulant matrix
triangular matrix
diagonally dominant matrix
block matrix — matrix composed of smaller matrices
stieltjes matrix — symmetric positive definite with non-positive off-diagonal entries
hilbert matrix — example of a matrix which is extremely ill-conditioned (and thus difficult to handle)
wilkinson matrix — example of a symmetric tridiagonal matrix with pairs of nearly, but not exactly, equal eigenvalues
convergent matrix — square matrix whose successive powers approach the zero matrix
algorithms for matrix multiplication:
strassen algorithm
coppersmith–winograd algorithm
cannon's algorithm — a distributed algorithm, especially suitable for processors laid out in a 2d grid
freivalds' algorithm — a randomized algorithm for checking the result of a multiplication
matrix decompositions:
lu decomposition — lower triangular times upper triangular
qr decomposition — orthogonal matrix times triangular matrix
rrqr factorization — rank-revealing qr factorization, can be used to compute rank of a matrix
polar decomposition — unitary matrix times positive-semidefinite hermitian matrix
decompositions by similarity:
eigendecomposition — decomposition in terms of eigenvectors and eigenvalues
jordan normal form — bidiagonal matrix of a certain form; generalizes the eigendecomposition
weyr canonical form — permutation of jordan normal form
jordan–chevalley decomposition — sum of commuting nilpotent matrix and diagonalizable matrix
schur decomposition — similarity transform bringing the matrix to a triangular matrix
singular value decomposition — unitary matrix times diagonal matrix times unitary matrix
matrix splitting — expressing a given matrix as a sum or difference of matrices


=== solving systems of linear equations ===
gaussian elimination
row echelon form — matrix in which all entries below a nonzero entry are zero
bareiss algorithm — variant which ensures that all entries remain integers if the initial matrix has integer entries
tridiagonal matrix algorithm — simplified form of gaussian elimination for tridiagonal matrices
lu decomposition — write a matrix as a product of an upper- and a lower-triangular matrix
crout matrix decomposition
lu reduction — a special parallelized version of a lu decomposition algorithm
block lu decomposition
cholesky decomposition — for solving a system with a positive definite matrix
minimum degree algorithm
symbolic cholesky decomposition
iterative refinement — procedure to turn an inaccurate solution in a more accurate one
direct methods for sparse matrices:
frontal solver — used in finite element methods
nested dissection — for symmetric matrices, based on graph partitioning
levinson recursion — for toeplitz matrices
spike algorithm — hybrid parallel solver for narrow-banded matrices
cyclic reduction — eliminate even or odd rows or columns, repeat
iterative methods:
jacobi method
gauss–seidel method
successive over-relaxation (sor) — a technique to accelerate the gauss–seidel method
symmetric successive over-relaxation (ssor) — variant of sor for symmetric matrices
backfitting algorithm — iterative procedure used to fit a generalized additive model, often equivalent to gauss–seidel
modified richardson iteration
conjugate gradient method (cg) — assumes that the matrix is positive definite
derivation of the conjugate gradient method
nonlinear conjugate gradient method — generalization for nonlinear optimization problems
biconjugate gradient method (bicg)
biconjugate gradient stabilized method (bicgstab) — variant of bicg with better convergence
conjugate residual method — similar to cg but only assumed that the matrix is symmetric
generalized minimal residual method (gmres) — based on the arnoldi iteration
chebyshev iteration — avoids inner products but needs bounds on the spectrum
stone's method (sip — strongly implicit procedure) — uses an incomplete lu decomposition
kaczmarz method
preconditioner
incomplete cholesky factorization — sparse approximation to the cholesky factorization
incomplete lu factorization — sparse approximation to the lu factorization
uzawa iteration — for saddle node problems
underdetermined and overdetermined systems (systems that have no or more than one solution):
numerical computation of null space — find all solutions of an underdetermined system
moore–penrose pseudoinverse — for finding solution with smallest 2-norm (for underdetermined systems) or smallest residual
sparse approximation — for finding the sparsest solution (i.e., the solution with as many zeros as possible)


=== eigenvalue algorithms ===
eigenvalue algorithm — a numerical algorithm for locating the eigenvalues of a matrix

power iteration
inverse iteration
rayleigh quotient iteration
arnoldi iteration — based on krylov subspaces
lanczos algorithm — arnoldi, specialized for positive-definite matrices
block lanczos algorithm — for when matrix is over a finite field
qr algorithm
jacobi eigenvalue algorithm — select a small submatrix which can be diagonalized exactly, and repeat
jacobi rotation — the building block, almost a givens rotation
jacobi method for complex hermitian matrices
divide-and-conquer eigenvalue algorithm
folded spectrum method
lobpcg — locally optimal block preconditioned conjugate gradient method
eigenvalue perturbation — stability of eigenvalues under perturbations of the matrix


=== other concepts and algorithms ===
orthogonalization algorithms:
gram–schmidt process
householder transformation
householder operator — analogue of householder transformation for general inner product spaces
givens rotation
krylov subspace
block matrix pseudoinverse
bidiagonalization
cuthill–mckee algorithm — permutes rows/columns in sparse matrix to yield a narrow band matrix
in-place matrix transposition — computing the transpose of a matrix without using much additional storage
pivot element — entry in a matrix on which the algorithm concentrates
matrix-free methods — methods that only access the matrix by evaluating matrix-vector products


== interpolation and approximation ==
interpolation — construct a function going through some given data points

nearest-neighbor interpolation — takes the value of the nearest neighbor


=== polynomial interpolation ===
polynomial interpolation — interpolation by polynomials

linear interpolation
runge's phenomenon
vandermonde matrix
chebyshev polynomials
chebyshev nodes
lebesgue constant (interpolation)
different forms for the interpolant:
newton polynomial
divided differences
neville's algorithm — for evaluating the interpolant; based on the newton form
lagrange polynomial
bernstein polynomial — especially useful for approximation
brahmagupta's interpolation formula — seventh-century formula for quadratic interpolation
extensions to multiple dimensions:
bilinear interpolation
trilinear interpolation
bicubic interpolation
tricubic interpolation
padua points — set of points in r2 with unique polynomial interpolant and minimal growth of lebesgue constant
hermite interpolation
birkhoff interpolation
abel–goncharov interpolation


=== spline interpolation ===
spline interpolation — interpolation by piecewise polynomials

spline (mathematics) — the piecewise polynomials used as interpolants
perfect spline — polynomial spline of degree m whose mth derivate is ±1
cubic hermite spline
centripetal catmull–rom spline — special case of cubic hermite splines without self-intersections or cusps
monotone cubic interpolation
hermite spline
bézier curve
de casteljau's algorithm
composite bézier curve
generalizations to more dimensions:
bézier triangle — maps a triangle to r3
bézier surface — maps a square to r3
b-spline
box spline — multivariate generalization of b-splines
truncated power function
de boor's algorithm — generalizes de casteljau's algorithm
non-uniform rational b-spline (nurbs)
t-spline — can be thought of as a nurbs surface for which a row of control points is allowed to terminate
kochanek–bartels spline
coons patch — type of manifold parametrization used to smoothly join other surfaces together
m-spline — a non-negative spline
i-spline — a monotone spline, defined in terms of m-splines
smoothing spline — a spline fitted smoothly to noisy data
blossom (functional) — a unique, affine, symmetric map associated to a polynomial or spline
see also: list of numerical computational geometry topics


=== trigonometric interpolation ===
trigonometric interpolation — interpolation by trigonometric polynomials

discrete fourier transform — can be viewed as trigonometric interpolation at equidistant points
relations between fourier transforms and fourier series
fast fourier transform (fft) — a fast method for computing the discrete fourier transform
bluestein's fft algorithm
bruun's fft algorithm
cooley–tukey fft algorithm
split-radix fft algorithm — variant of cooley–tukey that uses a blend of radices 2 and 4
goertzel algorithm
prime-factor fft algorithm
rader's fft algorithm
bit-reversal permutation — particular permutation of vectors with 2m entries used in many ffts.
butterfly diagram
twiddle factor — the trigonometric constant coefficients that are multiplied by the data
cyclotomic fast fourier transform — for fft over finite fields
methods for computing discrete convolutions with finite impulse response filters using the fft:
overlap–add method
overlap–save method
sigma approximation
dirichlet kernel — convolving any function with the dirichlet kernel yields its trigonometric interpolant
gibbs phenomenon


=== other interpolants ===
simple rational approximation
polynomial and rational function modeling — comparison of polynomial and rational interpolation
wavelet
continuous wavelet
transfer matrix
see also: list of functional analysis topics, list of wavelet-related transforms
inverse distance weighting
radial basis function (rbf) — a function of the form ƒ(x) = φ(|x−x0|)
polyharmonic spline — a commonly used radial basis function
thin plate spline — a specific polyharmonic spline: r2 log r
hierarchical rbf
subdivision surface — constructed by recursively subdividing a piecewise linear interpolant
catmull–clark subdivision surface
doo–sabin subdivision surface
loop subdivision surface
slerp (spherical linear interpolation) — interpolation between two points on a sphere
generalized quaternion interpolation — generalizes slerp for interpolation between more than two quaternions
irrational base discrete weighted transform
nevanlinna–pick interpolation — interpolation by analytic functions in the unit disc subject to a bound
pick matrix — the nevanlinna–pick interpolation has a solution if this matrix is positive semi-definite
multivariate interpolation — the function being interpolated depends on more than one variable
barnes interpolation — method for two-dimensional functions using gaussians common in meteorology
coons surface — combination of linear interpolation and bilinear interpolation
lanczos resampling — based on convolution with a sinc function
natural neighbor interpolation
nearest neighbor value interpolation
pde surface
transfinite interpolation — constructs function on planar domain given its values on the boundary
trend surface analysis — based on low-order polynomials of spatial coordinates; uses scattered observations
method based on polynomials are listed under polynomial interpolation


=== approximation theory ===
approximation theory

orders of approximation
lebesgue's lemma
curve fitting
vector field reconstruction
modulus of continuity — measures smoothness of a function
least squares (function approximation) — minimizes the error in the l2-norm
minimax approximation algorithm — minimizes the maximum error over an interval (the l∞-norm)
equioscillation theorem — characterizes the best approximation in the l∞-norm
unisolvent point set — function from given function space is determined uniquely by values on such a set of points
stone–weierstrass theorem — continuous functions can be approximated uniformly by polynomials, or certain other function spaces
approximation by polynomials:
linear approximation
bernstein polynomial — basis of polynomials useful for approximating a function
bernstein's constant — error when approximating |x| by a polynomial
remez algorithm — for constructing the best polynomial approximation in the l∞-norm
bernstein's inequality (mathematical analysis) — bound on maximum of derivative of polynomial in unit disk
mergelyan's theorem — generalization of stone–weierstrass theorem for polynomials
müntz–szász theorem — variant of stone–weierstrass theorem for polynomials if some coefficients have to be zero
bramble–hilbert lemma — upper bound on lp error of polynomial approximation in multiple dimensions
discrete chebyshev polynomials — polynomials orthogonal with respect to a discrete measure
favard's theorem — polynomials satisfying suitable 3-term recurrence relations are orthogonal polynomials
approximation by fourier series / trigonometric polynomials:
jackson's inequality — upper bound for best approximation by a trigonometric polynomial
bernstein's theorem (approximation theory) — a converse to jackson's inequality
fejér's theorem — cesàro means of partial sums of fourier series converge uniformly for continuous periodic functions
erdős–turán inequality — bounds distance between probability and lebesgue measure in terms of fourier coefficients
different approximations:
moving least squares
padé approximant
padé table — table of padé approximants
hartogs–rosenthal theorem — continuous functions can be approximated uniformly by rational functions on a set of lebesgue measure zero
szász–mirakyan operator — approximation by e−n xk on a semi-infinite interval
szász–mirakjan–kantorovich operator
baskakov operator — generalize bernstein polynomials, szász–mirakyan operators, and lupas operators
favard operator — approximation by sums of gaussians
surrogate model — application: replacing a function that is hard to evaluate by a simpler function
constructive function theory — field that studies connection between degree of approximation and smoothness
universal differential equation — differential–algebraic equation whose solutions can approximate any continuous function
fekete problem — find n points on a sphere that minimize some kind of energy
carleman's condition — condition guaranteeing that a measure is uniquely determined by its moments
krein's condition — condition that exponential sums are dense in weighted l2 space
lethargy theorem — about distance of points in a metric space from members of a sequence of subspaces
wirtinger's representation and projection theorem
journals:
constructive approximation
journal of approximation theory


=== miscellaneous ===
extrapolation
linear predictive analysis — linear extrapolation
unisolvent functions — functions for which the interpolation problem has a unique solution
regression analysis
isotonic regression
curve-fitting compaction
interpolation (computer graphics)


== finding roots of nonlinear equations ==
see #numerical linear algebra for linear equationsroot-finding algorithm — algorithms for solving the equation f(x) = 0

general methods:
bisection method — simple and robust; linear convergence
lehmer–schur algorithm — variant for complex functions
fixed-point iteration
newton's method — based on linear approximation around the current iterate; quadratic convergence
kantorovich theorem — gives a region around solution such that newton's method converges
newton fractal — indicates which initial condition converges to which root under newton iteration
quasi-newton method — uses an approximation of the jacobian:
broyden's method — uses a rank-one update for the jacobian
symmetric rank-one — a symmetric (but not necessarily positive definite) rank-one update of the jacobian
davidon–fletcher–powell formula — update of the jacobian in which the matrix remains positive definite
broyden–fletcher–goldfarb–shanno algorithm — rank-two update of the jacobian in which the matrix remains positive definite
limited-memory bfgs method — truncated, matrix-free variant of bfgs method suitable for large problems
steffensen's method — uses divided differences instead of the derivative
secant method — based on linear interpolation at last two iterates
false position method — secant method with ideas from the bisection method
muller's method — based on quadratic interpolation at last three iterates
sidi's generalized secant method — higher-order variants of secant method
inverse quadratic interpolation — similar to muller's method, but interpolates the inverse
brent's method — combines bisection method, secant method and inverse quadratic interpolation
ridders' method — fits a linear function times an exponential to last two iterates and their midpoint
halley's method — uses f, f' and f''; achieves the cubic convergence
householder's method — uses first d derivatives to achieve order d + 1; generalizes newton's and halley's method
methods for polynomials:
aberth method
bairstow's method
durand–kerner method
graeffe's method
jenkins–traub algorithm — fast, reliable, and widely used
laguerre's method
splitting circle method
analysis:
wilkinson's polynomial
numerical continuation — tracking a root as one parameter in the equation changes
piecewise linear continuation


== optimization ==
mathematical optimization — algorithm for finding maxima or minima of a given function


=== basic concepts ===
active set
candidate solution
constraint (mathematics)
constrained optimization — studies optimization problems with constraints
binary constraint — a constraint that involves exactly two variables
corner solution
feasible region — contains all solutions that satisfy the constraints but may not be optimal
global optimum and local optimum
maxima and minima
slack variable
continuous optimization
discrete optimization


=== linear programming ===
linear programming (also treats integer programming) — objective function and constraints are linear

algorithms for linear programming:
simplex algorithm
bland's rule — rule to avoid cycling in the simplex method
klee–minty cube — perturbed (hyper)cube; simplex method has exponential complexity on such a domain
criss-cross algorithm — similar to the simplex algorithm
big m method — variation of simplex algorithm for problems with both "less than" and "greater than" constraints
interior point method
ellipsoid method
karmarkar's algorithm
mehrotra predictor–corrector method
column generation
k-approximation of k-hitting set — algorithm for specific lp problems (to find a weighted hitting set)
linear complementarity problem
decompositions:
benders' decomposition
dantzig–wolfe decomposition
theory of two-level planning
variable splitting
basic solution (linear programming) — solution at vertex of feasible region
fourier–motzkin elimination
hilbert basis (linear programming) — set of integer vectors in a convex cone which generate all integer vectors in the cone
lp-type problem
linear inequality
vertex enumeration problem — list all vertices of the feasible set


=== convex optimization ===
convex optimization

quadratic programming
linear least squares (mathematics)
total least squares
frank–wolfe algorithm
sequential minimal optimization — breaks up large qp problems into a series of smallest possible qp problems
bilinear program
basis pursuit — minimize l1-norm of vector subject to linear constraints
basis pursuit denoising (bpdn) — regularized version of basis pursuit
in-crowd algorithm — algorithm for solving basis pursuit denoising
linear matrix inequality
conic optimization
semidefinite programming
second-order cone programming
sum-of-squares optimization
quadratic programming (see above)
bregman method — row-action method for strictly convex optimization problems
proximal gradient method — use splitting of objective function in sum of possible non-differentiable pieces
subgradient method — extension of steepest descent for problems with a non-differentiable objective function
biconvex optimization — generalization where objective function and constraint set can be biconvex


=== nonlinear programming ===
nonlinear programming — the most general optimization problem in the usual framework

special cases of nonlinear programming:
see linear programming and convex optimization above
geometric programming — problems involving signomials or posynomials
signomial — similar to polynomials, but exponents need not be integers
posynomial — a signomial with positive coefficients
quadratically constrained quadratic program
linear-fractional programming — objective is ratio of linear functions, constraints are linear
fractional programming — objective is ratio of nonlinear functions, constraints are linear
nonlinear complementarity problem (ncp) — find x such that x ≥ 0, f(x) ≥ 0 and xt f(x) = 0
least squares — the objective function is a sum of squares
non-linear least squares
gauss–newton algorithm
bhhh algorithm — variant of gauss–newton in econometrics
generalized gauss–newton method — for constrained nonlinear least-squares problems
levenberg–marquardt algorithm
iteratively reweighted least squares (irls) — solves a weighted least-squares problem at every iteration
partial least squares — statistical techniques similar to principal components analysis
non-linear iterative partial least squares (nipls)
mathematical programming with equilibrium constraints — constraints include variational inequalities or complementarities
univariate optimization:
golden section search
successive parabolic interpolation — based on quadratic interpolation through the last three iterates
general algorithms:
concepts:
descent direction
guess value — the initial guess for a solution with which an algorithm starts
line search
backtracking line search
wolfe conditions
gradient method — method that uses the gradient as the search direction
gradient descent
stochastic gradient descent
landweber iteration — mainly used for ill-posed problems
successive linear programming (slp) — replace problem by a linear programming problem, solve that, and repeat
sequential quadratic programming (sqp) — replace problem by a quadratic programming problem, solve that, and repeat
newton's method in optimization
see also under newton algorithm in the section finding roots of nonlinear equations
nonlinear conjugate gradient method
derivative-free methods
coordinate descent — move in one of the coordinate directions
adaptive coordinate descent — adapt coordinate directions to objective function
random coordinate descent — randomized version
nelder–mead method
pattern search (optimization)
powell's method — based on conjugate gradient descent
rosenbrock methods — derivative-free method, similar to nelder–mead but with guaranteed convergence
augmented lagrangian method — replaces constrained problems by unconstrained problems with a term added to the objective function
ternary search
tabu search
guided local search — modification of search algorithms which builds up penalties during a search
reactive search optimization (rso) — the algorithm adapts its parameters automatically
mm algorithm — majorize-minimization, a wide framework of methods
least absolute deviations
expectation–maximization algorithm
ordered subset expectation maximization
nearest neighbor search
space mapping — uses "coarse" (ideal or low-fidelity) and "fine" (practical or high-fidelity) models


=== optimal control and infinite-dimensional optimization ===
optimal control

pontryagin's minimum principle — infinite-dimensional version of lagrange multipliers
costate equations — equation for the "lagrange multipliers" in pontryagin's minimum principle
hamiltonian (control theory) — minimum principle says that this function should be minimized
types of problems:
linear-quadratic regulator — system dynamics is a linear differential equation, objective is quadratic
linear-quadratic-gaussian control (lqg) — system dynamics is a linear sde with additive noise, objective is quadratic
optimal projection equations — method for reducing dimension of lqg control problem
algebraic riccati equation — matrix equation occurring in many optimal control problems
bang–bang control — control that switches abruptly between two states
covector mapping principle
differential dynamic programming — uses locally-quadratic models of the dynamics and cost functions
dnss point — initial state for certain optimal control problems with multiple optimal solutions
legendre–clebsch condition — second-order condition for solution of optimal control problem
pseudospectral optimal control
bellman pseudospectral method — based on bellman's principle of optimality
chebyshev pseudospectral method — uses chebyshev polynomials (of the first kind)
flat pseudospectral method — combines ross–fahroo pseudospectral method with differential flatness
gauss pseudospectral method — uses collocation at the legendre–gauss points
legendre pseudospectral method — uses legendre polynomials
pseudospectral knotting method — generalization of pseudospectral methods in optimal control
ross–fahroo pseudospectral method — class of pseudospectral method including chebyshev, legendre and knotting
ross–fahroo lemma — condition to make discretization and duality operations commute
ross' π lemma — there is fundamental time constant within which a control solution must be computed for controllability and stability
sethi model — optimal control problem modelling advertisinginfinite-dimensional optimization

semi-infinite programming — infinite number of variables and finite number of constraints, or other way around
shape optimization, topology optimization — optimization over a set of regions
topological derivative — derivative with respect to changing in the shape
generalized semi-infinite programming — finite number of variables, infinite number of constraints


=== uncertainty and randomness ===
approaches to deal with uncertainty:
markov decision process
partially observable markov decision process
robust optimization
wald's maximin model
scenario optimization — constraints are uncertain
stochastic approximation
stochastic optimization
stochastic programming
stochastic gradient descent
random optimization algorithms:
random search — choose a point randomly in ball around current iterate
simulated annealing
adaptive simulated annealing — variant in which the algorithm parameters are adjusted during the computation.
great deluge algorithm
mean field annealing — deterministic variant of simulated annealing
bayesian optimization — treats objective function as a random function and places a prior over it
evolutionary algorithm
differential evolution
evolutionary programming
genetic algorithm, genetic programming
genetic algorithms in economics
mcacea (multiple coordinated agents coevolution evolutionary algorithm) — uses an evolutionary algorithm for every agent
simultaneous perturbation stochastic approximation (spsa)
luus–jaakola
particle swarm optimization
stochastic tunneling
harmony search — mimicks the improvisation process of musicians
see also the section monte carlo method


=== theoretical aspects ===
convex analysis — function f such that f(tx + (1 − t)y) ≥ tf(x) + (1 − t)f(y) for t ∈ [0,1]
pseudoconvex function — function f such that ∇f · (y − x) ≥ 0 implies f(y) ≥ f(x)
quasiconvex function — function f such that f(tx + (1 − t)y) ≤ max(f(x), f(y)) for t ∈ [0,1]
subderivative
geodesic convexity — convexity for functions defined on a riemannian manifold
duality (optimization)
weak duality — dual solution gives a bound on the primal solution
strong duality — primal and dual solutions are equivalent
shadow price
dual cone and polar cone
duality gap — difference between primal and dual solution
fenchel's duality theorem — relates minimization problems with maximization problems of convex conjugates
perturbation function — any function which relates to primal and dual problems
slater's condition — sufficient condition for strong duality to hold in a convex optimization problem
total dual integrality — concept of duality for integer linear programming
wolfe duality — for when objective function and constraints are differentiable
farkas' lemma
karush–kuhn–tucker conditions (kkt) — sufficient conditions for a solution to be optimal
fritz john conditions — variant of kkt conditions
lagrange multiplier
lagrange multipliers on banach spaces
semi-continuity
complementarity theory — study of problems with constraints of the form ⟨u, v⟩ = 0
mixed complementarity problem
mixed linear complementarity problem
lemke's algorithm — method for solving (mixed) linear complementarity problems
danskin's theorem — used in the analysis of minimax problems
maximum theorem — the maximum and maximizer are continuous as function of parameters, under some conditions
no free lunch in search and optimization
relaxation (approximation) — approximating a given problem by an easier problem by relaxing some constraints
lagrangian relaxation
linear programming relaxation — ignoring the integrality constraints in a linear programming problem
self-concordant function
reduced cost — cost for increasing a variable by a small amount
hardness of approximation — computational complexity of getting an approximate solution


=== applications ===
in geometry:
geometric median — the point minimizing the sum of distances to a given set of points
chebyshev center — the centre of the smallest ball containing a given set of points
in statistics:
iterated conditional modes — maximizing joint probability of markov random field
response surface methodology — used in the design of experiments
automatic label placement
compressed sensing — reconstruct a signal from knowledge that it is sparse or compressible
cutting stock problem
demand optimization
destination dispatch — an optimization technique for dispatching elevators
energy minimization
entropy maximization
highly optimized tolerance
hyperparameter optimization
inventory control problem
newsvendor model
extended newsvendor model
assemble-to-order system
linear programming decoding
linear search problem — find a point on a line by moving along the line
low-rank approximation — find best approximation, constraint is that rank of some matrix is smaller than a given number
meta-optimization — optimization of the parameters in an optimization method
multidisciplinary design optimization
optimal computing budget allocation — maximize the overall simulation efficiency for finding an optimal decision
paper bag problem
process optimization
recursive economics — individuals make a series of two-period optimization decisions over time.
stigler diet
space allocation problem
stress majorization
trajectory optimization
transportation theory
wing-shape optimization


=== miscellaneous ===
combinatorial optimization
dynamic programming
bellman equation
hamilton–jacobi–bellman equation — continuous-time analogue of bellman equation
backward induction — solving dynamic programming problems by reasoning backwards in time
optimal stopping — choosing the optimal time to take a particular action
odds algorithm
robbins' problem
global optimization:
brst algorithm
mcs algorithm
multi-objective optimization — there are multiple conflicting objectives
benson's algorithm — for linear vector optimization problems
bilevel optimization — studies problems in which one problem is embedded in another
optimal substructure
dykstra's projection algorithm — finds a point in intersection of two convex sets
algorithmic concepts:
barrier function
penalty method
trust region
test functions for optimization:
rosenbrock function — two-dimensional function with a banana-shaped valley
himmelblau's function — two-dimensional with four local minima, defined by 
  
    
      
        f
        (
        x
        ,
        y
        )
        =
        (
        
          x
          
            2
          
        
        +
        y
        −
        11
        
          )
          
            2
          
        
        +
        (
        x
        +
        
          y
          
            2
          
        
        −
        7
        
          )
          
            2
          
        
      
    
    {\displaystyle f(x,y)=(x^{2}+y-11)^{2}+(x+y^{2}-7)^{2}}
  
rastrigin function — two-dimensional function with many local minima
shekel function — multimodal and multidimensional
mathematical optimization society


== numerical quadrature (integration) ==
numerical integration — the numerical evaluation of an integral

rectangle method — first-order method, based on (piecewise) constant approximation
trapezoidal rule — second-order method, based on (piecewise) linear approximation
simpson's rule — fourth-order method, based on (piecewise) quadratic approximation
adaptive simpson's method
boole's rule — sixth-order method, based on the values at five equidistant points
newton–cotes formulas — generalizes the above methods
romberg's method — richardson extrapolation applied to trapezium rule
gaussian quadrature — highest possible degree with given number of points
chebyshev–gauss quadrature — extension of gaussian quadrature for integrals with weight (1 − x2)±1/2 on [−1, 1]
gauss–hermite quadrature — extension of gaussian quadrature for integrals with weight exp(−x2) on [−∞, ∞]
gauss–jacobi quadrature — extension of gaussian quadrature for integrals with weight (1 − x)α (1 + x)β on [−1, 1]
gauss–laguerre quadrature — extension of gaussian quadrature for integrals with weight exp(−x) on [0, ∞]
gauss–kronrod quadrature formula — nested rule based on gaussian quadrature
gauss–kronrod rules
tanh-sinh quadrature — variant of gaussian quadrature which works well with singularities at the end points
clenshaw–curtis quadrature — based on expanding the integrand in terms of chebyshev polynomials
adaptive quadrature — adapting the subintervals in which the integration interval is divided depending on the integrand
monte carlo integration — takes random samples of the integrand
see also #monte carlo method
quantized state systems method (qss) — based on the idea of state quantization
lebedev quadrature — uses a grid on a sphere with octahedral symmetry
sparse grid
coopmans approximation
numerical differentiation — for fractional-order integrals
numerical smoothing and differentiation
adjoint state method — approximates gradient of a function in an optimization problem
euler–maclaurin formula


== numerical methods for ordinary differential equations ==
numerical methods for ordinary differential equations — the numerical solution of ordinary differential equations (odes)

euler method — the most basic method for solving an ode
explicit and implicit methods — implicit methods need to solve an equation at every step
backward euler method — implicit variant of the euler method
trapezoidal rule — second-order implicit method
runge–kutta methods — one of the two main classes of methods for initial-value problems
midpoint method — a second-order method with two stages
heun's method — either a second-order method with two stages, or a third-order method with three stages
bogacki–shampine method — a third-order method with four stages (fsal) and an embedded fourth-order method
cash–karp method — a fifth-order method with six stages and an embedded fourth-order method
dormand–prince method — a fifth-order method with seven stages (fsal) and an embedded fourth-order method
runge–kutta–fehlberg method — a fifth-order method with six stages and an embedded fourth-order method
gauss–legendre method — family of a-stable method with optimal order based on gaussian quadrature
butcher group — algebraic formalism involving rooted trees for analysing runge–kutta methods
list of runge–kutta methods
linear multistep method — the other main class of methods for initial-value problems
backward differentiation formula — implicit methods of order 2 to 6; especially suitable for stiff equations
numerov's method — fourth-order method for equations of the form 
  
    
      
        
          y
          ″
        
        =
        f
        (
        t
        ,
        y
        )
      
    
    {\displaystyle y''=f(t,y)}
  
predictor–corrector method — uses one method to approximate solution and another one to increase accuracy
general linear methods — a class of methods encapsulating linear multistep and runge-kutta methods
bulirsch–stoer algorithm — combines the midpoint method with richardson extrapolation to attain arbitrary order
exponential integrator — based on splitting ode in a linear part, which is solved exactly, and a nonlinear part
methods designed for the solution of odes from classical physics:
newmark-beta method — based on the extended mean-value theorem
verlet integration — a popular second-order method
leapfrog integration — another name for verlet integration
beeman's algorithm — a two-step method extending the verlet method
dynamic relaxation
geometric integrator — a method that preserves some geometric structure of the equation
symplectic integrator — a method for the solution of hamilton's equations that preserves the symplectic structure
variational integrator — symplectic integrators derived using the underlying variational principle
semi-implicit euler method — variant of euler method which is symplectic when applied to separable hamiltonians
energy drift — phenomenon that energy, which should be conserved, drifts away due to numerical errors
other methods for initial value problems (ivps):
bi-directional delay line
partial element equivalent circuit
methods for solving two-point boundary value problems (bvps):
shooting method
direct multiple shooting method — divides interval in several subintervals and applies the shooting method on each subinterval
methods for solving differential-algebraic equations (daes), i.e., odes with constraints:
constraint algorithm — for solving newton's equations with constraints
pantelides algorithm — for reducing the index of a dea
methods for solving stochastic differential equations (sdes):
euler–maruyama method — generalization of the euler method for sdes
milstein method — a method with strong order one
runge–kutta method (sde) — generalization of the family of runge–kutta methods for sdes
methods for solving integral equations:
nyström method — replaces the integral with a quadrature rule
analysis:
truncation error (numerical integration) — local and global truncation errors, and their relationships
lady windermere's fan (mathematics) — telescopic identity relating local and global truncation errors
stiff equation — roughly, an ode for which unstable methods need a very short step size, but stable methods do not
l-stability — method is a-stable and stability function vanishes at infinity
adaptive stepsize — automatically changing the step size when that seems advantageous
parareal -- a parallel-in-time integration algorithm


== numerical methods for partial differential equations ==
numerical partial differential equations — the numerical solution of partial differential equations (pdes)


=== finite difference methods ===
finite difference method — based on approximating differential operators with difference operators

finite difference — the discrete analogue of a differential operator
finite difference coefficient — table of coefficients of finite-difference approximations to derivatives
discrete laplace operator — finite-difference approximation of the laplace operator
eigenvalues and eigenvectors of the second derivative — includes eigenvalues of discrete laplace operator
kronecker sum of discrete laplacians — used for laplace operator in multiple dimensions
discrete poisson equation — discrete analogue of the poisson equation using the discrete laplace operator
stencil (numerical analysis) — the geometric arrangements of grid points affected by a basic step of the algorithm
compact stencil — stencil which only uses a few grid points, usually only the immediate and diagonal neighbours
higher-order compact finite difference scheme
non-compact stencil — any stencil that is not compact
five-point stencil — two-dimensional stencil consisting of a point and its four immediate neighbours on a rectangular grid
finite difference methods for heat equation and related pdes:
ftcs scheme (forward-time central-space) — first-order explicit
crank–nicolson method — second-order implicit
finite difference methods for hyperbolic pdes like the wave equation:
lax–friedrichs method — first-order explicit
lax–wendroff method — second-order explicit
maccormack method — second-order explicit
upwind scheme
upwind differencing scheme for convection — first-order scheme for convection–diffusion problems
lax–wendroff theorem — conservative scheme for hyperbolic system of conservation laws converges to the weak solution
alternating direction implicit method (adi) — update using the flow in x-direction and then using flow in y-direction
nonstandard finite difference scheme
specific applications:
finite difference methods for option pricing
finite-difference time-domain method — a finite-difference method for electrodynamics


=== finite element methods, gradient discretisation methods ===
finite element method — based on a discretization of the space of solutions
gradient discretisation method — based on both the discretization of the solution and of its gradient

finite element method in structural mechanics — a physical approach to finite element methods
galerkin method — a finite element method in which the residual is orthogonal to the finite element space
discontinuous galerkin method — a galerkin method in which the approximate solution is not continuous
rayleigh–ritz method — a finite element method based on variational principles
spectral element method — high-order finite element methods
hp-fem — variant in which both the size and the order of the elements are automatically adapted
examples of finite elements:
bilinear quadrilateral element — also known as the q4 element
constant strain triangle element (cst) — also known as the t3 element
quadratic quadrilateral element — also known as the q8 element
barsoum elements
direct stiffness method — a particular implementation of the finite element method, often used in structural analysis
trefftz method
finite element updating
extended finite element method — puts functions tailored to the problem in the approximation space
functionally graded elements — elements for describing functionally graded materials
superelement — particular grouping of finite elements, employed as a single element
interval finite element method — combination of finite elements with interval arithmetic
discrete exterior calculus — discrete form of the exterior calculus of differential geometry
modal analysis using fem — solution of eigenvalue problems to find natural vibrations
céa's lemma — solution in the finite-element space is an almost best approximation in that space of the true solution
patch test (finite elements) — simple test for the quality of a finite element
mafelap (mathematics of finite elements and applications) — international conference held at brunel university
nafems — not-for-profit organisation that sets and maintains standards in computer-aided engineering analysis
multiphase topology optimisation — technique based on finite elements for determining optimal composition of a mixture
interval finite element
applied element method — for simulation of cracks and structural collapse
wood–armer method — structural analysis method based on finite elements used to design reinforcement for concrete slabs
isogeometric analysis — integrates finite elements into conventional nurbs-based cad design tools
loubignac iteration
stiffness matrix — finite-dimensional analogue of differential operator
combination with meshfree methods:
weakened weak form — form of a pde that is weaker than the standard weak form
g space — functional space used in formulating the weakened weak form
smoothed finite element method
variational multiscale method
list of finite element software packages


=== other methods ===
spectral method — based on the fourier transformation
pseudo-spectral method
method of lines — reduces the pde to a large system of ordinary differential equations
boundary element method (bem) — based on transforming the pde to an integral equation on the boundary of the domain
interval boundary element method — a version using interval arithmetics
analytic element method — similar to the boundary element method, but the integral equation is evaluated analytically
finite volume method — based on dividing the domain in many small domains; popular in computational fluid dynamics
godunov's scheme — first-order conservative scheme for fluid flow, based on piecewise constant approximation
muscl scheme — second-order variant of godunov's scheme
ausm — advection upstream splitting method
flux limiter — limits spatial derivatives (fluxes) in order to avoid spurious oscillations
riemann solver — a solver for riemann problems (a conservation law with piecewise constant data)
properties of discretization schemes — finite volume methods can be conservative, bounded, etc.
discrete element method — a method in which the elements can move freely relative to each other
extended discrete element method — adds properties such as strain to each particle
movable cellular automaton — combination of cellular automata with discrete elements
meshfree methods — does not use a mesh, but uses a particle view of the field
discrete least squares meshless method — based on minimization of weighted summation of the squared residual
diffuse element method
finite pointset method — represent continuum by a point cloud
moving particle semi-implicit method
method of fundamental solutions (mfs) — represents solution as linear combination of fundamental solutions
variants of mfs with source points on the physical boundary:
boundary knot method (bkm)
boundary particle method (bpm)
regularized meshless method (rmm)
singular boundary method (sbm)
methods designed for problems from electromagnetics:
finite-difference time-domain method — a finite-difference method
rigorous coupled-wave analysis — semi-analytical fourier-space method based on floquet's theorem
transmission-line matrix method (tlm) — based on analogy between electromagnetic field and mesh of transmission lines
uniform theory of diffraction — specifically designed for scattering problems
particle-in-cell — used especially in fluid dynamics
multiphase particle-in-cell method — considers solid particles as both numerical particles and fluid
high-resolution scheme
shock capturing method
vorticity confinement — for vortex-dominated flows in fluid dynamics, similar to shock capturing
split-step method
fast marching method
orthogonal collocation
lattice boltzmann methods — for the solution of the navier-stokes equations
roe solver — for the solution of the euler equation
relaxation (iterative method) — a method for solving elliptic pdes by converting them to evolution equations
broad classes of methods:
mimetic methods — methods that respect in some sense the structure of the original problem
multiphysics — models consisting of various submodels with different physics
immersed boundary method — for simulating elastic structures immersed within fluids
multisymplectic integrator — extension of symplectic integrators, which are for odes
stretched grid method — for problems solution that can be related to an elastic grid behavior.


=== techniques for improving these methods ===
multigrid method — uses a hierarchy of nested meshes to speed up the methods
domain decomposition methods — divides the domain in a few subdomains and solves the pde on these subdomains
additive schwarz method
abstract additive schwarz method — abstract version of additive schwarz without reference to geometric information
balancing domain decomposition method (bdd) — preconditioner for symmetric positive definite matrices
balancing domain decomposition by constraints (bddc) — further development of bdd
finite element tearing and interconnect (feti)
feti-dp — further development of feti
fictitious domain method — preconditioner constructed with a structured mesh on a fictitious domain of simple shape
mortar methods — meshes on subdomain do not mesh
neumann–dirichlet method — combines neumann problem on one subdomain with dirichlet problem on other subdomain
neumann–neumann methods — domain decomposition methods that use neumann problems on the subdomains
poincaré–steklov operator — maps tangential electric field onto the equivalent electric current
schur complement method — early and basic method on subdomains that do not overlap
schwarz alternating method — early and basic method on subdomains that overlap
coarse space — variant of the problem which uses a discretization with fewer degrees of freedom
adaptive mesh refinement — uses the computed solution to refine the mesh only where necessary
fast multipole method — hierarchical method for evaluating particle-particle interactions
perfectly matched layer — artificial absorbing layer for wave equations, used to implement absorbing boundary conditions


=== grids and meshes ===
grid classification / types of mesh:
polygon mesh — consists of polygons in 2d or 3d
triangle mesh — consists of triangles in 2d or 3d
triangulation (geometry) — subdivision of given region in triangles, or higher-dimensional analogue
nonobtuse mesh — mesh in which all angles are less than or equal to 90°
point-set triangulation — triangle mesh such that given set of point are all a vertex of a triangle
polygon triangulation — triangle mesh inside a polygon
delaunay triangulation — triangulation such that no vertex is inside the circumcentre of a triangle
constrained delaunay triangulation — generalization of the delaunay triangulation that forces certain required segments into the triangulation
pitteway triangulation — for any point, triangle containing it has nearest neighbour of the point as a vertex
minimum-weight triangulation — triangulation of minimum total edge length
kinetic triangulation — a triangulation that moves over time
triangulated irregular network
quasi-triangulation — subdivision into simplices, where vertices are not points but arbitrary sloped line segments
volume mesh — consists of three-dimensional shapes
regular grid — consists of congruent parallelograms, or higher-dimensional analogue
unstructured grid
geodesic grid — isotropic grid on a sphere
mesh generation
image-based meshing — automatic procedure of generating meshes from 3d image data
marching cubes — extracts a polygon mesh from a scalar field
parallel mesh generation
ruppert's algorithm — creates quality delauney triangularization from piecewise linear data
subdivisions:
apollonian network — undirected graph formed by recursively subdividing a triangle
barycentric subdivision — standard way of dividing arbitrary convex polygons into triangles, or the higher-dimensional analogue
improving an existing mesh:
chew's second algorithm — improves delauney triangularization by refining poor-quality triangles
laplacian smoothing — improves polynomial meshes by moving the vertices
jump-and-walk algorithm — for finding triangle in a mesh containing a given point
spatial twist continuum — dual representation of a mesh consisting of hexahedra
pseudotriangle — simply connected region between any three mutually tangent convex sets
simplicial complex — all vertices, line segments, triangles, tetrahedra, ..., making up a mesh


=== analysis ===
lax equivalence theorem — a consistent method is convergent if and only if it is stable
courant–friedrichs–lewy condition — stability condition for hyperbolic pdes
von neumann stability analysis — all fourier components of the error should be stable
numerical diffusion — diffusion introduced by the numerical method, above to that which is naturally present
false diffusion
numerical dispersion
numerical resistivity — the same, with resistivity instead of diffusion
weak formulation — a functional-analytic reformulation of the pde necessary for some methods
total variation diminishing — property of schemes that do not introduce spurious oscillations
godunov's theorem — linear monotone schemes can only be of first order
motz's problem — benchmark problem for singularity problems


== monte carlo method ==
variants of the monte carlo method:
direct simulation monte carlo
quasi-monte carlo method
markov chain monte carlo
metropolis–hastings algorithm
multiple-try metropolis — modification which allows larger step sizes
wang and landau algorithm — extension of metropolis monte carlo
equation of state calculations by fast computing machines — 1953 article proposing the metropolis monte carlo algorithm
multicanonical ensemble — sampling technique that uses metropolis–hastings to compute integrals
gibbs sampling
coupling from the past
reversible-jump markov chain monte carlo
dynamic monte carlo method
kinetic monte carlo
gillespie algorithm
particle filter
auxiliary particle filter
reverse monte carlo
demon algorithm
pseudo-random number sampling
inverse transform sampling — general and straightforward method but computationally expensive
rejection sampling — sample from a simpler distribution but reject some of the samples
ziggurat algorithm — uses a pre-computed table covering the probability distribution with rectangular segments
for sampling from a normal distribution:
box–muller transform
marsaglia polar method
convolution random number generator — generates a random variable as a sum of other random variables
indexed search
variance reduction techniques:
antithetic variates
control variates
importance sampling
stratified sampling
vegas algorithm
low-discrepancy sequence
constructions of low-discrepancy sequences
event generator
parallel tempering
umbrella sampling — improves sampling in physical systems with significant energy barriers
hybrid monte carlo
ensemble kalman filter — recursive filter suitable for problems with a large number of variables
transition path sampling
walk-on-spheres method — to generate exit-points of brownian motion from bounded domains
applications:
ensemble forecasting — produce multiple numerical predictions from slightly initial conditions or parameters
bond fluctuation model — for simulating the conformation and dynamics of polymer systems
iterated filtering
metropolis light transport
monte carlo localization — estimates the position and orientation of a robot
monte carlo methods for electron transport
monte carlo method for photon transport
monte carlo methods in finance
monte carlo methods for option pricing
quasi-monte carlo methods in finance
monte carlo molecular modeling
path integral molecular dynamics — incorporates feynman path integrals
quantum monte carlo
diffusion monte carlo — uses a green function to solve the schrödinger equation
gaussian quantum monte carlo
path integral monte carlo
reptation monte carlo
variational monte carlo
methods for simulating the ising model:
swendsen–wang algorithm — entire sample is divided into equal-spin clusters
wolff algorithm — improvement of the swendsen–wang algorithm
metropolis–hastings algorithm
auxiliary field monte carlo — computes averages of operators in many-body quantum mechanical problems
cross-entropy method — for multi-extremal optimization and importance sampling
also see the list of statistics topics


== applications ==
computational physics
computational electromagnetics
computational fluid dynamics (cfd)
numerical methods in fluid mechanics
large eddy simulation
smoothed-particle hydrodynamics
aeroacoustic analogy — used in numerical aeroacoustics to reduce sound sources to simple emitter types
stochastic eulerian lagrangian method — uses eulerian description for fluids and lagrangian for structures
explicit algebraic stress model
computational magnetohydrodynamics (cmhd) — studies electrically conducting fluids
climate model
numerical weather prediction
geodesic grid
celestial mechanics
numerical model of the solar system
quantum jump method — used for simulating open quantum systems, operates on wave function
dynamic design analysis method (ddam) — for evaluating effect of underwater explosions on equipment
computational chemistry
cell lists
coupled cluster
density functional theory
diis — direct inversion in (or of) the iterative subspace
computational sociology
computational statistics


== software ==
for a large list of software, see the list of numerical-analysis software.


== journals ==
acta numerica
mathematics of computation (published by the american mathematical society)
journal of computational and applied mathematics
bit numerical mathematics
numerische mathematik
journals from the society for industrial and applied mathematics
siam journal on numerical analysis
siam journal on scientific computing


== researchers ==
cleve moler
gene h. golub
james h. wilkinson
margaret h. wright
nicholas j. higham
nick trefethen
peter lax
richard s. varga
ulrich w. kulisch
vladik kreinovich