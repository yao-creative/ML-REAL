bayesian optimization is a sequential design strategy for global optimization of black-box functions that does not assume any functional forms. it is usually employed to optimize expensive-to-evaluate functions.


== history ==
the term is generally attributed to jonas mockus and is coined in his work from a series of publications on global optimization in the 1970s and 1980s.


== strategy ==

bayesian optimization is typically used on problems of the form 
  
    
      
        
          max
          
            x
            ∈
            a
          
        
        f
        (
        x
        )
      
    
    {\textstyle \max _{x\in a}f(x)}
  , where 
  
    
      
        a
      
    
    {\textstyle a}
   is a set of points, 
  
    
      
        x
      
    
    {\textstyle x}
  , which rely upon less than 20 dimensions (
  
    
      
        
          
            r
          
          
            d
          
        
        ,
        d
        ≤
        20
      
    
    {\textstyle \mathbb {r} ^{d},d\leq 20}
  ), and whose membership can easily be evaluated. bayesian optimization is particularly advantageous for problems where 
  
    
      
        f
        (
        x
        )
      
    
    {\textstyle f(x)}
   is difficult to evaluate due to its computational cost. the objective function, 
  
    
      
        f
      
    
    {\textstyle f}
  , is continuous and takes the form of some unknown structure, referred to as a "black box". upon its evaluation, only 
  
    
      
        f
        (
        x
        )
      
    
    {\textstyle f(x)}
   is observed and its derivatives are not evaluated.since the objective function is unknown, the bayesian strategy is to treat it as a random function and place a prior over it. the prior captures beliefs about the behavior of the function. after gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. the posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point.
there are several methods used to define the prior/posterior distribution over the objective function. the most common two methods use gaussian processes in a method called kriging. another less expensive method uses the parzen-tree estimator to construct two distributions for 'high' and 'low' points, and then finds the location that maximizes the expected improvement.standard bayesian optimization relies upon each 
  
    
      
        x
        ∈
        a
      
    
    {\displaystyle x\in a}
   being easy to evaluate, and problems that deviate from this assumption are known as exotic bayesian optimization problems. optimization problems can become exotic if it is known that there is noise, the evaluations are being done in parallel, the quality of evaluations relies upon a tradeoff between difficulty and accuracy, the presence of random environmental conditions, or if the evaluation involves derivatives.


== examples ==
examples of acquisition functions include probability of improvement, expected improvement, bayesian expected losses, upper confidence bounds (ucb), thompson sampling and hybrids of these. they all trade-off exploration and exploitation so as to minimize the number of function queries. as such, bayesian optimization is well suited for functions that are expensive to evaluate.


== solution methods ==
the maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer. acquisition functions are typically well-behaved and are 
maximized using a numerical optimization technique, such as newton's method or quasi-newton methods like the broyden–fletcher–goldfarb–shanno algorithm.


== applications ==
the approach has been applied to solve a wide range of problems, including learning to rank, computer graphics and visual design, robotics, sensor networks, automatic algorithm configuration, automatic machine learning toolboxes, reinforcement learning, planning, visual attention, architecture configuration in deep learning, static program analysis, experimental particle physics, chemistry, material design, and drug development.


== see also ==
multi-armed bandit
thompson sampling
global optimization
bayesian experimental design


== references ==


== external links ==
gpyopt, python open-source library for bayesian optimization based on gpy.
bayesopt, an efficient implementation in c/c++ with support for python, matlab and octave.
spearmint, a python implementation focused on parallel and cluster computing.
smac, a java implementation of random-forest-based bayesian optimization for general algorithm configuration.
parbayesianoptimization, a high performance, parallel implementation of bayesian optimization with gaussian processes in r.
pybo, a python implementation of modular bayesian optimization.
bayesopt.m, a matlab implementation of bayesian optimization with or without constraints.
moe moe is a python/c++/cuda implementation of bayesian global optimization using gaussian processes.
sigopt sigopt offers bayesian global optimization as a saas service focused on enterprise use cases.
mind foundry optaas offers bayesian global optimization via web-services with flexible parameter constraints.
bayeso, a python implementation of bayesian optimization.
botorch, a modular and modern pytorch-based open-source library for bayesian optimization research with support for gpytorch.
gpflowopt, a tensorflow-based open-source package for bayesian optimization.
trieste, an open-source bayesian optimization toolbox built on tensorflow and gpflow.