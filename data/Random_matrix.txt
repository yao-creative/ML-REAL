in probability theory and mathematical physics, a random matrix is a matrix-valued random variable—that is, a matrix in which some or all elements are random variables. many important properties of physical systems can be represented mathematically as matrix problems. for example, the thermal conductivity of a lattice can be computed from the dynamical matrix of the particle-particle interactions within the lattice.


== applications ==


=== physics ===
in nuclear physics, random matrices were introduced by eugene wigner to model the nuclei of heavy atoms. he postulated that the spacings between the lines in the spectrum of a heavy atom nucleus should resemble the spacings between the eigenvalues of a random matrix, and should depend only on the symmetry class of the underlying evolution. in solid-state physics, random matrices model the behaviour of large disordered hamiltonians in the mean field approximation.
in quantum chaos, the bohigas–giannoni–schmit (bgs) conjecture asserts that the spectral statistics of quantum systems whose classical counterparts exhibit chaotic behaviour are described by random matrix theory.in quantum optics, transformations described by random unitary matrices are crucial for demonstrating the advantage of quantum over classical computation (see, e.g., the boson sampling model). moreover, such random unitary transformations can be directly implemented in an optical circuit, by mapping their parameters to optical circuit components (that is beam splitters and phase shifters).random matrix theory has also found applications to the chiral dirac operator in quantum chromodynamics, quantum gravity in two dimensions, mesoscopic physics, spin-transfer torque, the fractional quantum hall effect, anderson localization, quantum dots, and superconductors


=== mathematical statistics and numerical analysis ===
in multivariate statistics,  random matrices were introduced by john wishart for statistical analysis of large samples; see estimation of covariance matrices.
significant results have been shown that extend the classical scalar chernoff, bernstein, and hoeffding inequalities to the largest eigenvalues of finite sums of random hermitian matrices.  corollary results are derived for the maximum singular values of rectangular matrices.
in numerical analysis, random matrices have been used since the work of john von neumann and herman goldstine to describe computation errors in operations such as matrix multiplication. see also for more recent results.


=== number theory ===
in number theory, the distribution of zeros of the riemann zeta function (and other l-functions) is modeled by the distribution of eigenvalues of certain random matrices. the connection was first discovered by hugh montgomery and freeman j. dyson. it is connected to the hilbert–pólya conjecture.


=== theoretical neuroscience ===
in the field of theoretical neuroscience, random matrices are increasingly used to model the network of synaptic connections between neurons in the brain.  dynamical models of neuronal networks with random connectivity matrix were shown to exhibit a phase transition to chaos  when the variance of the synaptic weights crosses a critical value, at the limit of infinite system size. relating the statistical properties of the spectrum of biologically inspired random matrix models to the dynamical behavior of randomly connected neural networks is an intensive research topic.


=== optimal control ===
in optimal control theory, the evolution of n  state variables through time depends at any time on their own values and on the values of k control variables. with linear evolution, matrices of coefficients appear in the state equation (equation of evolution). in some problems the values of the parameters in these matrices are not known with certainty, in which case there are random matrices in the state equation and the problem is known as one of stochastic control.: ch. 13  a key result in the case of linear-quadratic control with stochastic matrices is that the certainty equivalence principle does not apply: while in the absence of multiplier uncertainty (that is, with only additive uncertainty) the optimal policy with a quadratic loss function coincides with what would be decided if the uncertainty were ignored, this no longer holds in the presence of random coefficients in the state equation.


== gaussian ensembles ==
the most studied random matrix ensembles are the gaussian ensembles.
the gaussian unitary ensemble 
  
    
      
        
          gue
        
        (
        n
        )
      
    
    {\displaystyle {\text{gue}}(n)}
   is described by the gaussian measure with density

  
    
      
        
          
            1
            
              z
              
                
                  gue
                
                (
                n
                )
              
            
          
        
        
          e
          
            −
            
              
                n
                2
              
            
            
              t
              r
            
            
              h
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {1}{z_{{\text{gue}}(n)}}}e^{-{\frac {n}{2}}\mathrm {tr} h^{2}}}
  on the space of 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
   hermitian matrices 
  
    
      
        h
        =
        (
        
          h
          
            i
            j
          
        
        
          )
          
            i
            ,
            j
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle h=(h_{ij})_{i,j=1}^{n}}
  . here

  
    
      
        
          z
          
            
              gue
            
            (
            n
            )
          
        
        =
        
          2
          
            n
            
              /
            
            2
          
        
        
          π
          
            
              
                1
                2
              
            
            
              n
              
                2
              
            
          
        
      
    
    {\displaystyle z_{{\text{gue}}(n)}=2^{n/2}\pi ^{{\frac {1}{2}}n^{2}}}
   is a normalization constant, chosen so that the integral of the density is equal to one. the term unitary refers to the fact that the distribution is invariant under unitary conjugation.
the gaussian unitary ensemble models hamiltonians lacking time-reversal symmetry.
the gaussian orthogonal ensemble 
  
    
      
        
          goe
        
        (
        n
        )
      
    
    {\displaystyle {\text{goe}}(n)}
   is described by the gaussian measure with density

  
    
      
        
          
            1
            
              z
              
                
                  goe
                
                (
                n
                )
              
            
          
        
        
          e
          
            −
            
              
                n
                4
              
            
            
              t
              r
            
            
              h
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {1}{z_{{\text{goe}}(n)}}}e^{-{\frac {n}{4}}\mathrm {tr} h^{2}}}
  on the space of n × n real symmetric matrices h = (hij)ni,j=1. its distribution is invariant under orthogonal conjugation, and it models hamiltonians with time-reversal symmetry.
the gaussian symplectic ensemble 
  
    
      
        
          gse
        
        (
        n
        )
      
    
    {\displaystyle {\text{gse}}(n)}
   is described by the gaussian measure with density

  
    
      
        
          
            1
            
              z
              
                
                  gse
                
                (
                n
                )
              
            
          
        
        
          e
          
            −
            n
            
              t
              r
            
            
              h
              
                2
              
            
          
        
        
      
    
    {\displaystyle {\frac {1}{z_{{\text{gse}}(n)}}}e^{-n\mathrm {tr} h^{2}}\,}
  on the space of n × n hermitian quaternionic matrices, e.g. symmetric square matrices composed of quaternions, h = (hij)ni,j=1. its distribution is invariant under conjugation by the symplectic group, and it models hamiltonians with time-reversal symmetry but no rotational symmetry.
the gaussian ensembles goe, gue and gse are often denoted by their dyson index, β = 1 for goe, β = 2 for gue, and β = 4 for gse. this index counts the number of real components per matrix element. the ensembles as defined here have gaussian distributed matrix elements with mean ⟨hij⟩ = 0, and two-point correlations given by

  
    
      
        ⟨
        
          h
          
            i
            j
          
        
        
          h
          
            m
            n
          
          
            ∗
          
        
        ⟩
        =
        ⟨
        
          h
          
            i
            j
          
        
        
          h
          
            n
            m
          
        
        ⟩
        =
        
          
            1
            n
          
        
        
          δ
          
            i
            m
          
        
        
          δ
          
            j
            n
          
        
        +
        
          
            
              2
              −
              β
            
            
              n
              β
            
          
        
        
          δ
          
            i
            n
          
        
        
          δ
          
            j
            m
          
        
      
    
    {\displaystyle \langle h_{ij}h_{mn}^{*}\rangle =\langle h_{ij}h_{nm}\rangle ={\frac {1}{n}}\delta _{im}\delta _{jn}+{\frac {2-\beta }{n\beta }}\delta _{in}\delta _{jm}}
  ,from which all higher correlations follow by isserlis' theorem.
the joint probability density for the eigenvalues λ1,λ2,...,λn of gue/goe/gse is given by

  
    
      
        
          
            1
            
              z
              
                β
                ,
                n
              
            
          
        
        
          ∏
          
            k
            =
            1
          
          
            n
          
        
        
          e
          
            −
            
              
                β
                4
              
            
            
              λ
              
                k
              
              
                2
              
            
          
        
        
          ∏
          
            i
            <
            j
          
        
        
          
            |
            
              
                λ
                
                  j
                
              
              −
              
                λ
                
                  i
                
              
            
            |
          
          
            β
          
        
         
        ,
        
        (
        1
        )
      
    
    {\displaystyle {\frac {1}{z_{\beta ,n}}}\prod _{k=1}^{n}e^{-{\frac {\beta }{4}}\lambda _{k}^{2}}\prod _{i<j}\left|\lambda _{j}-\lambda _{i}\right|^{\beta }~,\quad (1)}
  where zβ,n is a normalization constant which can be explicitly computed, see selberg integral. in the case of gue (β = 2), the formula (1) describes a determinantal point process. eigenvalues repel as the joint probability density has a zero (of 
  
    
      
        β
      
    
    {\displaystyle \beta }
  th order) for coinciding eigenvalues 
  
    
      
        
          λ
          
            j
          
        
        =
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{j}=\lambda _{i}}
  .
for the distribution of the largest eigenvalue for goe, gue and wishart matrices of finite dimensions, see.


=== distribution of level spacings ===
from the ordered sequence of eigenvalues 
  
    
      
        
          λ
          
            1
          
        
        <
        …
        <
        
          λ
          
            n
          
        
        <
        
          λ
          
            n
            +
            1
          
        
        <
        …
      
    
    {\displaystyle \lambda _{1}<\ldots <\lambda _{n}<\lambda _{n+1}<\ldots }
  , one defines the normalized spacings 
  
    
      
        s
        =
        (
        
          λ
          
            n
            +
            1
          
        
        −
        
          λ
          
            n
          
        
        )
        
          /
        
        ⟨
        s
        ⟩
      
    
    {\displaystyle s=(\lambda _{n+1}-\lambda _{n})/\langle s\rangle }
  , where 
  
    
      
        ⟨
        s
        ⟩
        =
        ⟨
        
          λ
          
            n
            +
            1
          
        
        −
        
          λ
          
            n
          
        
        ⟩
      
    
    {\displaystyle \langle s\rangle =\langle \lambda _{n+1}-\lambda _{n}\rangle }
   is the mean spacing. the probability distribution of spacings is approximately given by,

  
    
      
        
          p
          
            1
          
        
        (
        s
        )
        =
        
          
            π
            2
          
        
        s
        
        
          
            e
          
          
            −
            
              
                π
                4
              
            
            
              s
              
                2
              
            
          
        
      
    
    {\displaystyle p_{1}(s)={\frac {\pi }{2}}s\,\mathrm {e} ^{-{\frac {\pi }{4}}s^{2}}}
  for the orthogonal ensemble goe 
  
    
      
        β
        =
        1
      
    
    {\displaystyle \beta =1}
  ,                                    

  
    
      
        
          p
          
            2
          
        
        (
        s
        )
        =
        
          
            32
            
              π
              
                2
              
            
          
        
        
          s
          
            2
          
        
        
          
            e
          
          
            −
            
              
                4
                π
              
            
            
              s
              
                2
              
            
          
        
      
    
    {\displaystyle p_{2}(s)={\frac {32}{\pi ^{2}}}s^{2}\mathrm {e} ^{-{\frac {4}{\pi }}s^{2}}}
  for the unitary ensemble gue 
  
    
      
        β
        =
        2
      
    
    {\displaystyle \beta =2}
  ,  and                                  

  
    
      
        
          p
          
            4
          
        
        (
        s
        )
        =
        
          
            
              2
              
                18
              
            
            
              
                3
                
                  6
                
              
              
                π
                
                  3
                
              
            
          
        
        
          s
          
            4
          
        
        
          
            e
          
          
            −
            
              
                64
                
                  9
                  π
                
              
            
            
              s
              
                2
              
            
          
        
      
    
    {\displaystyle p_{4}(s)={\frac {2^{18}}{3^{6}\pi ^{3}}}s^{4}\mathrm {e} ^{-{\frac {64}{9\pi }}s^{2}}}
  for the symplectic ensemble gse 
  
    
      
        β
        =
        4
      
    
    {\displaystyle \beta =4}
  .
the numerical constants are such that 
  
    
      
        
          p
          
            β
          
        
        (
        s
        )
      
    
    {\displaystyle p_{\beta }(s)}
   is normalized:

  
    
      
        
          ∫
          
            0
          
          
            ∞
          
        
        d
        s
        
        
          p
          
            β
          
        
        (
        s
        )
        =
        1
      
    
    {\displaystyle \int _{0}^{\infty }ds\,p_{\beta }(s)=1}
  and the mean spacing is,

  
    
      
        
          ∫
          
            0
          
          
            ∞
          
        
        d
        s
        
        s
        
        
          p
          
            β
          
        
        (
        s
        )
        =
        1
        ,
      
    
    {\displaystyle \int _{0}^{\infty }ds\,s\,p_{\beta }(s)=1,}
  for 
  
    
      
        β
        =
        1
        ,
        2
        ,
        4
      
    
    {\displaystyle \beta =1,2,4}
  .


== generalizations ==
wigner matrices are random hermitian matrices 
  
    
      
        
          
            h
            
              n
            
          
          =
          (
          
            h
            
              n
            
          
          (
          i
          ,
          j
          )
          
            )
            
              i
              ,
              j
              =
              1
            
            
              n
            
          
        
      
    
    {\displaystyle \textstyle h_{n}=(h_{n}(i,j))_{i,j=1}^{n}}
   such that the entries

  
    
      
        
          {
          
            
              h
              
                n
              
            
            (
            i
            ,
            j
            )
             
            ,
            
            1
            ≤
            i
            ≤
            j
            ≤
            n
          
          }
        
      
    
    {\displaystyle \left\{h_{n}(i,j)~,\,1\leq i\leq j\leq n\right\}}
  above the main diagonal are independent random variables with zero mean and
have identical second moments.
invariant matrix ensembles are random hermitian matrices with density on the space of real symmetric/ hermitian/ quaternionic hermitian matrices, which is of the form

  
    
      
        
          
            
              1
              
                z
                
                  n
                
              
            
          
          
            e
            
              −
              n
              
                t
                r
              
              v
              (
              h
              )
            
          
           
          ,
        
      
    
    {\displaystyle \textstyle {\frac {1}{z_{n}}}e^{-n\mathrm {tr} v(h)}~,}
  
where the function v is called the potential.
the gaussian ensembles are the only common special cases of these two classes of random matrices.


== spectral theory of random matrices ==
the spectral theory of random matrices studies the distribution of the eigenvalues as the size of the matrix goes to infinity.


=== global regime ===
in the global regime, one is interested in the distribution of linear statistics of the form 
  
    
      
        
          n
          
            f
            ,
            h
          
        
        =
        
          n
          
            −
            1
          
        
        
          tr
        
        f
        (
        h
        )
      
    
    {\displaystyle n_{f,h}=n^{-1}{\text{tr}}f(h)}
  .


==== empirical spectral measure ====
the empirical spectral measure μh of h is defined by

  
    
      
        
          μ
          
            h
          
        
        (
        a
        )
        =
        
          
            1
            n
          
        
        
        #
        
          {
          
            
              eigenvalues of 
            
            h
            
               in 
            
            a
          
          }
        
        =
        
          n
          
            
              1
              
                a
              
            
            ,
            h
          
        
        ,
        
        a
        ⊂
        
          r
        
        .
      
    
    {\displaystyle \mu _{h}(a)={\frac {1}{n}}\,\#\left\{{\text{eigenvalues of }}h{\text{ in }}a\right\}=n_{1_{a},h},\quad a\subset \mathbb {r} .}
  usually, the limit of 
  
    
      
        
          μ
          
            h
          
        
      
    
    {\displaystyle \mu _{h}}
   is a deterministic measure; this is a particular case of self-averaging. the cumulative distribution function of the limiting measure is called the integrated density of states and is denoted n(λ). if the integrated density of states is differentiable, its derivative is called the density of states and is denoted ρ(λ).
the limit of the empirical spectral measure for wigner matrices was described by eugene wigner; see wigner semicircle distribution and wigner surmise. as far as sample covariance matrices are concerned, a theory was developed by marčenko and pastur.the limit of the empirical spectral measure of invariant matrix ensembles is described by a certain integral equation which arises from potential theory.


==== fluctuations ====
for the linear statistics nf,h = n−1 σ f(λj), one is also interested in the fluctuations about ∫ f(λ) dn(λ). for many classes of random matrices, a central limit theorem of the form

  
    
      
        
          
            
              
                n
                
                  f
                  ,
                  h
                
              
              −
              ∫
              f
              (
              λ
              )
              
              d
              n
              (
              λ
              )
            
            
              σ
              
                f
                ,
                n
              
            
          
        
        
          
            ⟶
            d
          
        
        n
        (
        0
        ,
        1
        )
      
    
    {\displaystyle {\frac {n_{f,h}-\int f(\lambda )\,dn(\lambda )}{\sigma _{f,n}}}{\overset {d}{\longrightarrow }}n(0,1)}
  is known, see, etc.


=== local regime ===
in the local regime, one is interested in the spacings between eigenvalues, and, more generally, in the joint distribution of eigenvalues in an interval of length of order 1/n. one distinguishes between bulk statistics, pertaining to intervals inside the support of the limiting spectral measure, and edge statistics, pertaining to intervals near the boundary of the support.


==== bulk statistics ====
formally, fix 
  
    
      
        
          λ
          
            0
          
        
      
    
    {\displaystyle \lambda _{0}}
   in the interior of the support of 
  
    
      
        n
        (
        λ
        )
      
    
    {\displaystyle n(\lambda )}
  . then consider the point process

  
    
      
        ξ
        (
        
          λ
          
            0
          
        
        )
        =
        
          ∑
          
            j
          
        
        δ
        
          
            (
          
        
        
          ⋅
        
        −
        n
        ρ
        (
        
          λ
          
            0
          
        
        )
        (
        
          λ
          
            j
          
        
        −
        
          λ
          
            0
          
        
        )
        
          
            )
          
        
         
        ,
      
    
    {\displaystyle \xi (\lambda _{0})=\sum _{j}\delta {\big (}{\cdot }-n\rho (\lambda _{0})(\lambda _{j}-\lambda _{0}){\big )}~,}
  where 
  
    
      
        
          λ
          
            j
          
        
      
    
    {\displaystyle \lambda _{j}}
   are the eigenvalues of the random matrix.
the point process 
  
    
      
        ξ
        (
        
          λ
          
            0
          
        
        )
      
    
    {\displaystyle \xi (\lambda _{0})}
   captures the statistical properties of eigenvalues in the vicinity of 
  
    
      
        
          λ
          
            0
          
        
      
    
    {\displaystyle \lambda _{0}}
  . for the gaussian ensembles, the limit of 
  
    
      
        ξ
        (
        
          λ
          
            0
          
        
        )
      
    
    {\displaystyle \xi (\lambda _{0})}
   is known; thus, for gue it is a determinantal point process with the kernel

  
    
      
        k
        (
        x
        ,
        y
        )
        =
        
          
            
              sin
              ⁡
              π
              (
              x
              −
              y
              )
            
            
              π
              (
              x
              −
              y
              )
            
          
        
      
    
    {\displaystyle k(x,y)={\frac {\sin \pi (x-y)}{\pi (x-y)}}}
  (the sine kernel).
the universality principle postulates that the limit of 
  
    
      
        ξ
        (
        
          λ
          
            0
          
        
        )
      
    
    {\displaystyle \xi (\lambda _{0})}
   as 
  
    
      
        n
        →
        ∞
      
    
    {\displaystyle n\to \infty }
   should depend only on the symmetry class of the random matrix (and neither on the specific model of random matrices nor on 
  
    
      
        
          λ
          
            0
          
        
      
    
    {\displaystyle \lambda _{0}}
  ). this was rigorously proved for several models of random matrices: for invariant matrix ensembles,
for wigner matrices,
et cet.


==== edge statistics ====
see tracy–widom distribution.


== correlation functions ==
the joint probability density of the eigenvalues of 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
   random hermitian matrices 
  
    
      
        m
        ∈
        
          
            h
          
          
            n
            ×
            n
          
        
      
    
    {\displaystyle m\in \mathbf {h} ^{n\times n}}
  , with partition functions of the form

  
    
      
        
          z
          
            n
          
        
        =
        
          ∫
          
            m
            ∈
            
              
                h
              
              
                n
                ×
                n
              
            
          
        
        d
        
          μ
          
            0
          
        
        (
        m
        )
        
          e
          
            
              tr
            
            (
            v
            (
            m
            )
            )
          
        
      
    
    {\displaystyle z_{n}=\int _{m\in \mathbf {h} ^{n\times n}}d\mu _{0}(m)e^{{\text{tr}}(v(m))}}
  where

  
    
      
        v
        (
        x
        )
        :=
        
          ∑
          
            j
            =
            1
          
          
            ∞
          
        
        
          v
          
            j
          
        
        
          x
          
            j
          
        
      
    
    {\displaystyle v(x):=\sum _{j=1}^{\infty }v_{j}x^{j}}
  and 
  
    
      
        d
        
          μ
          
            0
          
        
        (
        m
        )
      
    
    {\displaystyle d\mu _{0}(m)}
   is the standard lebesgue measure on the
space 
  
    
      
        
          
            h
          
          
            n
            ×
            n
          
        
      
    
    {\displaystyle \mathbf {h} ^{n\times n}}
   of hermitian 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
   matrices,
is given by

  
    
      
        
          p
          
            n
            ,
            v
          
        
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        =
        
          
            1
            
              z
              
                n
                ,
                v
              
            
          
        
        
          ∏
          
            i
            <
            j
          
        
        (
        
          x
          
            i
          
        
        −
        
          x
          
            j
          
        
        
          )
          
            2
          
        
        
          e
          
            −
            
              ∑
              
                i
              
            
            v
            (
            
              x
              
                i
              
            
            )
          
        
        .
      
    
    {\displaystyle p_{n,v}(x_{1},\dots ,x_{n})={\frac {1}{z_{n,v}}}\prod _{i<j}(x_{i}-x_{j})^{2}e^{-\sum _{i}v(x_{i})}.}
  the 
  
    
      
        k
      
    
    {\displaystyle k}
  -point correlation functions (or marginal distributions) 
are defined as

  
    
      
        
          r
          
            n
            ,
            v
          
          
            (
            k
            )
          
        
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            k
          
        
        )
        =
        
          
            
              n
              !
            
            
              (
              n
              −
              k
              )
              !
            
          
        
        
          ∫
          
            
              r
            
          
        
        d
        
          x
          
            k
            +
            1
          
        
        …
        
          ∫
          
            
              r
            
          
        
        d
        
          x
          
            n
          
        
        
        
          p
          
            n
            ,
            v
          
        
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        ,
      
    
    {\displaystyle r_{n,v}^{(k)}(x_{1},\dots ,x_{k})={\frac {n!}{(n-k)!}}\int _{\mathbf {r} }dx_{k+1}\dots \int _{\mathbb {r} }dx_{n}\,p_{n,v}(x_{1},x_{2},\dots ,x_{n}),}
  which are skew symmetric functions of their variables. 
in particular, the one-point correlation function, or density of states, is 

  
    
      
        
          r
          
            n
            ,
            v
          
          
            (
            1
            )
          
        
        (
        
          x
          
            1
          
        
        )
        =
        n
        
          ∫
          
            
              r
            
          
        
        d
        
          x
          
            2
          
        
        …
        
          ∫
          
            
              r
            
          
        
        d
        
          x
          
            n
          
        
        
        
          p
          
            n
            ,
            v
          
        
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        .
      
    
    {\displaystyle r_{n,v}^{(1)}(x_{1})=n\int _{\mathbb {r} }dx_{2}\dots \int _{\mathbf {r} }dx_{n}\,p_{n,v}(x_{1},x_{2},\dots ,x_{n}).}
  its integral over a borel set 
  
    
      
        b
        ⊂
        
          r
        
      
    
    {\displaystyle b\subset \mathbf {r} }
   gives the expected number of eigenvalues contained in 
  
    
      
        b
      
    
    {\displaystyle b}
  :

  
    
      
        
          ∫
          
            b
          
        
        
          r
          
            n
            ,
            v
          
          
            (
            1
            )
          
        
        (
        x
        )
        d
        x
        =
        
          e
        
        
          (
          
            #
            {
            
              eigenvalues in 
            
            b
            }
          
          )
        
        .
      
    
    {\displaystyle \int _{b}r_{n,v}^{(1)}(x)dx=\mathbf {e} \left(\#\{{\text{eigenvalues in }}b\}\right).}
  the following result expresses these correlation functions as determinants of the matrices formed 
from evaluating the appropriate integral kernel at the pairs 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
          x
          
            j
          
        
        )
      
    
    {\displaystyle (x_{i},x_{j})}
   of points appearing within the correlator.
theorem [dyson-mehta] 
for any  
  
    
      
        k
      
    
    {\displaystyle k}
  , 
  
    
      
        1
        ≤
        k
        ≤
        n
      
    
    {\displaystyle 1\leq k\leq n}
   the 
  
    
      
        k
      
    
    {\displaystyle k}
  -point correlation function 

  
    
      
        
          r
          
            n
            ,
            v
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle r_{n,v}^{(k)}}
   can be written as a determinant

  
    
      
        
          r
          
            n
            ,
            v
          
          
            (
            k
            )
          
        
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            k
          
        
        )
        =
        
          det
          
            1
            ≤
            i
            ,
            j
            ≤
            k
          
        
        
          (
          
            
              k
              
                n
                ,
                v
              
            
            (
            
              x
              
                i
              
            
            ,
            
              x
              
                j
              
            
            )
          
          )
        
        ,
      
    
    {\displaystyle r_{n,v}^{(k)}(x_{1},x_{2},\dots ,x_{k})=\det _{1\leq i,j\leq k}\left(k_{n,v}(x_{i},x_{j})\right),}
  where 
  
    
      
        
          k
          
            n
            ,
            v
          
        
        (
        x
        ,
        y
        )
      
    
    {\displaystyle k_{n,v}(x,y)}
   is the 
  
    
      
        n
      
    
    {\displaystyle n}
  th christoffel-darboux kernel

  
    
      
        
          k
          
            n
            ,
            v
          
        
        (
        x
        ,
        y
        )
        :=
        
          ∑
          
            k
            =
            0
          
          
            n
            −
            1
          
        
        
          ψ
          
            k
          
        
        (
        x
        )
        
          ψ
          
            k
          
        
        (
        y
        )
        ,
      
    
    {\displaystyle k_{n,v}(x,y):=\sum _{k=0}^{n-1}\psi _{k}(x)\psi _{k}(y),}
  associated to 
  
    
      
        v
      
    
    {\displaystyle v}
  , written in terms of the quasipolynomials 

  
    
      
        
          ψ
          
            k
          
        
        (
        x
        )
        =
        
          
            1
            
              
                
                  h
                  
                    k
                  
                
              
            
          
        
        
        
          p
          
            k
          
        
        (
        z
        )
        
        
          
            
              e
            
          
          
            −
            
              
                1
                2
              
            
            v
            (
            z
            )
          
        
        ,
      
    
    {\displaystyle \psi _{k}(x)={1 \over {\sqrt {h_{k}}}}\,p_{k}(z)\,{\rm {e}}^{-{1 \over 2}v(z)},}
  where 
  
    
      
        {
        
          p
          
            k
          
        
        (
        x
        )
        
          }
          
            k
            ∈
            
              n
            
          
        
      
    
    {\displaystyle \{p_{k}(x)\}_{k\in \mathbf {n} }}
   is a complete sequence
of monic polynomials, of the degrees indicated, satisying the orthogonilty conditions

  
    
      
        
          ∫
          
            
              r
            
          
        
        
          ψ
          
            j
          
        
        (
        x
        )
        
          ψ
          
            k
          
        
        (
        x
        )
        d
        x
        =
        
          δ
          
            j
            k
          
        
        .
      
    
    {\displaystyle \int _{\mathbf {r} }\psi _{j}(x)\psi _{k}(x)dx=\delta _{jk}.}
  


== other classes of random matrices ==


=== wishart matrices ===

wishart matrices are n × n random matrices of the form h = x x*, where x is an n × m random matrix (m ≥ n) with independent entries, and x* is its conjugate transpose. in the important special case considered by wishart, the entries of x are identically distributed gaussian random variables (either real or complex).
the limit of the empirical spectral measure of wishart matrices was found by vladimir marchenko and leonid pastur, see marchenko–pastur distribution.


=== random unitary matrices ===
see circular ensembles.


=== non-hermitian random matrices ===
see circular law.


== guide to references ==
books on random matrix theory:
survey articles on random matrix theory:
historic works:


== references ==


== external links ==
fyodorov, y. (2011). "random matrix theory". scholarpedia. 6 (3): 9886. bibcode:2011schpj...6.9886f. doi:10.4249/scholarpedia.9886.
weisstein, e. w. "random matrix". wolfram mathworld.