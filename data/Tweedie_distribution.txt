in probability and statistics, the tweedie distributions are a family of probability distributions which include the purely continuous normal, gamma and inverse gaussian distributions, the purely discrete scaled poisson distribution, and the class of compound poisson–gamma distributions which have positive mass at zero, but are otherwise continuous.
tweedie distributions are a special case of exponential dispersion models and are often used as distributions for generalized linear models.the tweedie distributions were named by bent jørgensen after maurice tweedie, a statistician and medical physicist at the university of liverpool, uk, who presented the first thorough study of these distributions in 1984.


== definitions ==
the (reproductive) tweedie distributions are defined as subfamily of (reproductive) exponential dispersion models (ed), with a special mean-variance relationship. 
a random variable y is tweedie distributed twp(μ, σ2), if 
  
    
      
        y
        ∼
        
          e
          d
        
        (
        μ
        ,
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle y\sim \mathrm {ed} (\mu ,\sigma ^{2})}
   with mean 
  
    
      
        μ
        =
        e
        ⁡
        (
        y
        )
      
    
    {\displaystyle \mu =\operatorname {e} (y)}
  , positive dispersion parameter 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
   and

  
    
      
        var
        ⁡
        (
        y
        )
        =
        
          σ
          
            2
          
        
        
        
          μ
          
            p
          
        
        ,
      
    
    {\displaystyle \operatorname {var} (y)=\sigma ^{2}\,\mu ^{p},}
  where 
  
    
      
        p
        ∈
        
          r
        
      
    
    {\displaystyle p\in \mathbf {r} }
   is called tweedie power parameter.
the probability distribution pθ,σ2 on the measurable sets a, is given by

  
    
      
        
          p
          
            θ
            ,
            
              σ
              
                2
              
            
          
        
        (
        y
        ∈
        a
        )
        =
        
          ∫
          
            a
          
        
        exp
        ⁡
        
          (
          
            
              
                θ
                ⋅
                z
                −
                
                  κ
                  
                    p
                  
                
                (
                θ
                )
              
              
                σ
                
                  2
                
              
            
          
          )
        
        ⋅
        
          ν
          
            λ
          
        
        
        (
        d
        z
        )
        ,
      
    
    {\displaystyle p_{\theta ,\sigma ^{2}}(y\in a)=\int _{a}\exp \left({\frac {\theta \cdot z-\kappa _{p}(\theta )}{\sigma ^{2}}}\right)\cdot \nu _{\lambda }\,(dz),}
  for some σ-finite measure νλ.
this representation uses the canonical parameter θ of an exponential dispersion model and cumulant function

  
    
      
        
          κ
          
            p
          
        
        (
        θ
        )
        =
        
          
            {
            
              
                
                  
                    
                      
                        α
                        −
                        1
                      
                      α
                    
                  
                  
                    
                      (
                      
                        
                          θ
                          
                            α
                            −
                            1
                          
                        
                      
                      )
                    
                    
                      α
                    
                  
                  ,
                
                
                  
                    for 
                  
                  p
                  ≠
                  1
                  ,
                  2
                
              
              
                
                  −
                  log
                  ⁡
                  (
                  −
                  θ
                  )
                  ,
                
                
                  
                    for 
                  
                  p
                  =
                  2
                
              
              
                
                  
                    e
                    
                      θ
                    
                  
                  ,
                
                
                  
                    for 
                  
                  p
                  =
                  1
                
              
            
            
          
        
      
    
    {\displaystyle \kappa _{p}(\theta )={\begin{cases}{\frac {\alpha -1}{\alpha }}\left({\frac {\theta }{\alpha -1}}\right)^{\alpha },&{\text{for }}p\neq 1,2\\-\log(-\theta ),&{\text{for }}p=2\\e^{\theta },&{\text{for }}p=1\end{cases}}}
  where we used 
  
    
      
        α
        =
        
          
            
              p
              −
              2
            
            
              p
              −
              1
            
          
        
      
    
    {\displaystyle \alpha ={\frac {p-2}{p-1}}}
  , or equivalently 
  
    
      
        p
        =
        
          
            
              α
              −
              2
            
            
              α
              −
              1
            
          
        
      
    
    {\displaystyle p={\frac {\alpha -2}{\alpha -1}}}
  .


== properties ==


=== additive exponential dispersion models ===
the models just described are in the reproductive form. an exponential dispersion model has always a dual: the additive form. if y is reproductive, then 
  
    
      
        z
        =
        λ
        y
      
    
    {\displaystyle z=\lambda y}
   with 
  
    
      
        λ
        =
        
          
            1
            
              σ
              
                2
              
            
          
        
      
    
    {\displaystyle \lambda ={\frac {1}{\sigma ^{2}}}}
   is in the additive form ed*(θ,λ), for tweedie tw*p(μ, λ). additive models have the property that the distribution of the sum of independent random variables,

  
    
      
        
          z
          
            +
          
        
        =
        
          z
          
            1
          
        
        +
        ⋯
        +
        
          z
          
            n
          
        
        ,
      
    
    {\displaystyle z_{+}=z_{1}+\cdots +z_{n},}
  for which zi ~ ed*(θ,λi) with fixed θ and various λ are members of the family of distributions with the same θ,

  
    
      
        
          z
          
            +
          
        
        ∼
        
          ed
          
            ∗
          
        
        ⁡
        (
        θ
        ,
        
          λ
          
            1
          
        
        +
        ⋯
        +
        
          λ
          
            n
          
        
        )
        .
      
    
    {\displaystyle z_{+}\sim \operatorname {ed} ^{*}(\theta ,\lambda _{1}+\cdots +\lambda _{n}).}
  


=== reproductive exponential dispersion models ===
a second class of exponential dispersion models exists designated by the random variable

  
    
      
        y
        =
        z
        
          /
        
        λ
        ∼
        ed
        ⁡
        (
        μ
        ,
        
          σ
          
            2
          
        
        )
        ,
      
    
    {\displaystyle y=z/\lambda \sim \operatorname {ed} (\mu ,\sigma ^{2}),}
  where σ2 = 1/λ, known as reproductive exponential dispersion models.  they have the property that for n independent random variables yi ~ ed(μ,σ2/wi), with weighting factors wi and

  
    
      
        w
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            i
          
        
        ,
      
    
    {\displaystyle w=\sum _{i=1}^{n}w_{i},}
  a weighted average of the variables gives,

  
    
      
        
          w
          
            −
            1
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          w
          
            i
          
        
        
          y
          
            i
          
        
        ∼
        ed
        ⁡
        (
        μ
        ,
        
          σ
          
            2
          
        
        
          /
        
        w
        )
        .
      
    
    {\displaystyle w^{-1}\sum _{i=1}^{n}w_{i}y_{i}\sim \operatorname {ed} (\mu ,\sigma ^{2}/w).}
  for reproductive models the weighted average of independent random variables with fixed μ and σ2 and various values for wi is a member of the family of distributions with same μ and σ2.
the tweedie exponential dispersion models are both additive and reproductive; we thus have the duality transformation

  
    
      
        y
        ↦
        z
        =
        y
        
          /
        
        
          σ
          
            2
          
        
        .
      
    
    {\displaystyle y\mapsto z=y/\sigma ^{2}.}
  


=== scale invariance ===
a third property of the tweedie models is that they are scale invariant:  for a reproductive exponential dispersion model twp(μ, σ2) and any positive constant c we have the property of closure under scale transformation,

  
    
      
        c
        
          tw
          
            p
          
        
        ⁡
        (
        μ
        ,
        
          σ
          
            2
          
        
        )
        =
        
          tw
          
            p
          
        
        ⁡
        (
        c
        μ
        ,
        
          c
          
            2
            −
            p
          
        
        
          σ
          
            2
          
        
        )
        .
      
    
    {\displaystyle c\operatorname {tw} _{p}(\mu ,\sigma ^{2})=\operatorname {tw} _{p}(c\mu ,c^{2-p}\sigma ^{2}).}
  


=== the tweedie power variance function ===
to define the variance function for exponential dispersion models we make use of the mean value mapping, the relationship between the canonical parameter θ and the mean μ.  it is defined by the function

  
    
      
        τ
        (
        θ
        )
        =
        
          κ
          
            ′
          
        
        (
        θ
        )
        =
        μ
        .
      
    
    {\displaystyle \tau (\theta )=\kappa ^{\prime }(\theta )=\mu .}
  with cumulative function 
  
    
      
        κ
        (
        θ
        )
      
    
    {\displaystyle \kappa (\theta )}
  .
the variance function v(μ) is constructed from the mean value mapping,

  
    
      
        v
        (
        μ
        )
        =
        
          τ
          
            ′
          
        
        [
        
          τ
          
            −
            1
          
        
        (
        μ
        )
        ]
        .
      
    
    {\displaystyle v(\mu )=\tau ^{\prime }[\tau ^{-1}(\mu )].}
  here the minus exponent in τ−1(μ) denotes an inverse function rather than a reciprocal.  the mean and variance of an additive random variable is then e(z) = λμ and var(z) = λv(μ).
scale invariance implies that the variance function obeys the relationship 
v(μ) = μ p.


=== the tweedie deviance ===
the unit deviance of a reproductive tweedie distribution is given by

  
    
      
        d
        (
        y
        ,
        μ
        )
        =
        
          
            {
            
              
                
                  (
                  y
                  −
                  μ
                  
                    )
                    
                      2
                    
                  
                  ,
                
                
                  
                    for 
                  
                  p
                  =
                  0
                
              
              
                
                  2
                  (
                  y
                  log
                  ⁡
                  (
                  y
                  
                    /
                  
                  μ
                  )
                  +
                  μ
                  −
                  y
                  )
                  ,
                
                
                  
                    for 
                  
                  p
                  =
                  1
                
              
              
                
                  2
                  (
                  log
                  ⁡
                  (
                  μ
                  
                    /
                  
                  y
                  )
                  +
                  y
                  
                    /
                  
                  μ
                  −
                  1
                  )
                  ,
                
                
                  
                    for 
                  
                  p
                  =
                  2
                
              
              
                
                  2
                  
                    (
                    
                      
                        
                          
                            max
                            (
                            y
                            ,
                            0
                            
                              )
                              
                                2
                                −
                                p
                              
                            
                          
                          
                            (
                            1
                            −
                            p
                            )
                            (
                            2
                            −
                            p
                            )
                          
                        
                      
                      −
                      
                        
                          
                            y
                            
                              μ
                              
                                1
                                −
                                p
                              
                            
                          
                          
                            1
                            −
                            p
                          
                        
                      
                      +
                      
                        
                          
                            μ
                            
                              2
                              −
                              p
                            
                          
                          
                            2
                            −
                            p
                          
                        
                      
                    
                    )
                  
                  ,
                
                
                  
                    else
                  
                
              
            
            
          
        
      
    
    {\displaystyle d(y,\mu )={\begin{cases}(y-\mu )^{2},&{\text{for }}p=0\\2(y\log(y/\mu )+\mu -y),&{\text{for }}p=1\\2(\log(\mu /y)+y/\mu -1),&{\text{for }}p=2\\2\left({\frac {\max(y,0)^{2-p}}{(1-p)(2-p)}}-{\frac {y\mu ^{1-p}}{1-p}}+{\frac {\mu ^{2-p}}{2-p}}\right),&{\text{else}}\end{cases}}}
  


=== the tweedie cumulant generating functions ===
the properties of exponential dispersion models give us two differential equations.  the first relates the mean value mapping and the variance function to each other,

  
    
      
        
          
            
              ∂
              
                τ
                
                  −
                  1
                
              
              (
              μ
              )
            
            
              ∂
              μ
            
          
        
        =
        
          
            1
            
              v
              (
              μ
              )
            
          
        
        .
      
    
    {\displaystyle {\frac {\partial \tau ^{-1}(\mu )}{\partial \mu }}={\frac {1}{v(\mu )}}.}
  the second shows how the mean value mapping is related to the cumulant function,

  
    
      
        
          
            
              ∂
              κ
              (
              θ
              )
            
            
              ∂
              θ
            
          
        
        =
        τ
        (
        θ
        )
        .
      
    
    {\displaystyle {\frac {\partial \kappa (\theta )}{\partial \theta }}=\tau (\theta ).}
  these equations can be solved to obtain the cumulant function for different cases of the tweedie models.  a cumulant generating function (cgf) may then be obtained from the cumulant function.  the additive cgf is generally specified by the equation

  
    
      
        
          k
          
            ∗
          
        
        (
        s
        )
        =
        log
        ⁡
        [
        e
        ⁡
        (
        
          e
          
            s
            z
          
        
        )
        ]
        =
        λ
        [
        κ
        (
        θ
        +
        s
        )
        −
        κ
        (
        θ
        )
        ]
        ,
      
    
    {\displaystyle k^{*}(s)=\log[\operatorname {e} (e^{sz})]=\lambda [\kappa (\theta +s)-\kappa (\theta )],}
  and the reproductive cgf by

  
    
      
        k
        (
        s
        )
        =
        log
        ⁡
        [
        e
        ⁡
        (
        
          e
          
            s
            y
          
        
        )
        ]
        =
        λ
        [
        κ
        (
        θ
        +
        s
        
          /
        
        λ
        )
        −
        κ
        (
        θ
        )
        ]
        ,
      
    
    {\displaystyle k(s)=\log[\operatorname {e} (e^{sy})]=\lambda [\kappa (\theta +s/\lambda )-\kappa (\theta )],}
  where s is the generating function variable.
for the additive tweedie models the cgfs take the form,

  
    
      
        
          k
          
            p
          
          
            ∗
          
        
        (
        s
        ;
        θ
        ,
        λ
        )
        =
        
          
            {
            
              
                
                  λ
                  
                    κ
                    
                      p
                    
                  
                  (
                  θ
                  )
                  [
                  (
                  1
                  +
                  s
                  
                    /
                  
                  θ
                  
                    )
                    
                      α
                    
                  
                  −
                  1
                  ]
                
                
                  
                  p
                  ≠
                  1
                  ,
                  2
                  ,
                
              
              
                
                  −
                  λ
                  log
                  ⁡
                  (
                  1
                  +
                  s
                  
                    /
                  
                  θ
                  )
                
                
                  
                  p
                  =
                  2
                  ,
                
              
              
                
                  λ
                  
                    e
                    
                      θ
                    
                  
                  (
                  
                    e
                    
                      s
                    
                  
                  −
                  1
                  )
                
                
                  
                  p
                  =
                  1
                  ,
                
              
            
            
          
        
      
    
    {\displaystyle k_{p}^{*}(s;\theta ,\lambda )={\begin{cases}\lambda \kappa _{p}(\theta )[(1+s/\theta )^{\alpha }-1]&\quad p\neq 1,2,\\-\lambda \log(1+s/\theta )&\quad p=2,\\\lambda e^{\theta }(e^{s}-1)&\quad p=1,\end{cases}}}
  and for the reproductive models,

  
    
      
        
          k
          
            p
          
        
        (
        s
        ;
        θ
        ,
        λ
        )
        =
        
          
            {
            
              
                
                  λ
                  
                    κ
                    
                      p
                    
                  
                  (
                  θ
                  )
                  
                    {
                    
                      [
                      1
                      +
                      s
                      
                        /
                      
                      (
                      θ
                      λ
                      )
                      
                        ]
                        
                          α
                        
                      
                      −
                      1
                    
                    }
                  
                
                
                  
                  p
                  ≠
                  1
                  ,
                  2
                  ,
                
              
              
                
                  −
                  λ
                  log
                  ⁡
                  [
                  1
                  +
                  s
                  
                    /
                  
                  (
                  θ
                  λ
                  )
                  ]
                
                
                  
                  p
                  =
                  2
                  ,
                
              
              
                
                  λ
                  
                    e
                    
                      θ
                    
                  
                  (
                  
                    e
                    
                      s
                      
                        /
                      
                      λ
                    
                  
                  −
                  1
                  )
                
                
                  
                  p
                  =
                  1.
                
              
            
            
          
        
      
    
    {\displaystyle k_{p}(s;\theta ,\lambda )={\begin{cases}\lambda \kappa _{p}(\theta )\left\{[1+s/(\theta \lambda )]^{\alpha }-1\right\}&\quad p\neq 1,2,\\-\lambda \log[1+s/(\theta \lambda )]&\quad p=2,\\\lambda e^{\theta }(e^{s/\lambda }-1)&\quad p=1.\end{cases}}}
  the additive and reproductive tweedie models are conventionally denoted by the symbols tw*p(θ,λ) and twp(θ,σ2), respectively.
the first and second derivatives of the cgfs, with s = 0, yields the mean and variance, respectively.  one can thus confirm that for the additive models the variance relates to the mean by the power law,

  
    
      
        
          v
          a
          r
        
        (
        z
        )
        ∝
        
          e
        
        (
        z
        
          )
          
            p
          
        
        .
      
    
    {\displaystyle \mathrm {var} (z)\propto \mathrm {e} (z)^{p}.}
  


=== the tweedie convergence theorem ===
the tweedie exponential dispersion models are fundamental in statistical theory consequent to their roles as foci of convergence for a wide range of statistical processes.  jørgensen et al proved a theorem that specifies the asymptotic behaviour of variance functions known as the tweedie convergence theorem. this theorem, in technical terms, is stated thus: the unit variance function is regular of order p at zero (or infinity) provided that v(μ) ~ c0μp for μ as it approaches zero (or infinity) for all real values of p and c0 > 0.  then for a unit variance function regular of order p at either zero or infinity and for

  
    
      
        p
        ∉
        (
        0
        ,
        1
        )
        ,
      
    
    {\displaystyle p\notin (0,1),}
  for any 
  
    
      
        μ
        >
        0
      
    
    {\displaystyle \mu >0}
  , and 
  
    
      
        
          σ
          
            2
          
        
        >
        0
      
    
    {\displaystyle \sigma ^{2}>0}
   we have

  
    
      
        
          c
          
            −
            1
          
        
        ed
        ⁡
        (
        c
        μ
        ,
        
          σ
          
            2
          
        
        
          c
          
            2
            −
            p
          
        
        )
        →
        t
        
          w
          
            p
          
        
        (
        μ
        ,
        
          c
          
            0
          
        
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle c^{-1}\operatorname {ed} (c\mu ,\sigma ^{2}c^{2-p})\rightarrow tw_{p}(\mu ,c_{0}\sigma ^{2})}
  as 
  
    
      
        c
        ↓
        0
      
    
    {\displaystyle c\downarrow 0}
   or 
  
    
      
        c
        →
        ∞
      
    
    {\displaystyle c\rightarrow \infty }
  , respectively, where the convergence is through values of c such that cμ is in the domain of θ and cp−2/σ2 is in the domain of λ.   the model must be infinitely divisible as c2−p approaches infinity.in nontechnical terms this theorem implies that any exponential dispersion model that asymptotically manifests a variance-to-mean power law is required to have a variance function that comes within the domain of attraction of a tweedie model.  almost all distribution functions with finite cumulant generating functions qualify as exponential dispersion models and most exponential dispersion models manifest variance functions of this form.  hence many probability distributions have variance functions that express this asymptotic behaviour, and  the tweedie distributions become foci of convergence for a wide range of data types.


== related distributions ==
the tweedie distributions include a number of familiar distributions as well as some unusual ones, each being specified by the domain of the index parameter.  we have the

extreme stable distribution, p < 0,
normal distribution, p = 0,
poisson distribution, p = 1,
compound poisson–gamma distribution, 1 < p < 2,
gamma distribution, p = 2,
positive stable distributions, 2 < p < 3,
inverse gaussian distribution, p = 3,
positive stable distributions, p > 3, and
extreme stable distributions, p = ∞.for 0 < p < 1 no tweedie model exists. note that all stable distributions mean actually generated by stable distributions.


== occurrence and applications ==


=== the tweedie models and taylor’s power law ===
taylor's law is an empirical law in ecology that relates the variance of the number of individuals of a species per unit area of habitat to the corresponding mean by a power-law relationship.  for the population count y with mean µ and variance var(y), taylor's law is written,

  
    
      
        var
        ⁡
        (
        y
        )
        =
        a
        
          μ
          
            p
          
        
        ,
      
    
    {\displaystyle \operatorname {var} (y)=a\mu ^{p},}
  where a and p are both positive constants.   since l. r. taylor described this law in 1961 there have been many different explanations offered to explain it, ranging from animal behavior, a random walk model, a stochastic birth, death, immigration and emigration model, to a consequence of equilibrium and non-equilibrium statistical mechanics.  no consensus exists as to an explanation for this model.
since taylor's law is mathematically identical to the variance-to-mean power law that characterizes the tweedie models, it seemed reasonable to use these models and the tweedie convergence theorem to explain the observed clustering of animals and plants associated with taylor's law.  the majority of the observed values for the power-law exponent p have fallen in the interval (1,2) and so the tweedie compound poisson–gamma distribution would seem applicable.  comparison of the empirical distribution function to the theoretical compound poisson–gamma distribution has provided a means to verify consistency of this hypothesis.whereas conventional models for taylor's law have tended to involve ad hoc animal behavioral or population dynamic assumptions, the tweedie convergence theorem would imply that taylor's law results from a general mathematical convergence effect much as how the central limit theorem governs the convergence behavior of certain types of random data.  indeed, any mathematical model, approximation or simulation that is designed to yield taylor's law (on the basis of this theorem) is required to converge to the form of the tweedie models.


=== tweedie convergence and 1/f noise ===
pink noise, or 1/f noise, refers to a pattern of noise characterized by a power-law relationship between its intensities s(f) at different frequencies f,

  
    
      
        s
        (
        f
        )
        ∝
        
          
            1
            
              f
              
                γ
              
            
          
        
        ,
      
    
    {\displaystyle s(f)\propto {\frac {1}{f^{\gamma }}},}
  where the dimensionless exponent γ ∈ [0,1].  it is found within a diverse number of natural processes.  many different explanations for 1/f noise exist, a widely held hypothesis is based on self-organized criticality where dynamical systems close to a critical point are thought to manifest scale-invariant spatial and/or temporal behavior.
in this subsection a mathematical connection between 1/f noise and the tweedie variance-to-mean power law will be described.  to begin, we first need to introduce self-similar processes:  for the sequence of numbers

  
    
      
        y
        =
        (
        
          y
          
            i
          
        
        :
        i
        =
        0
        ,
        1
        ,
        2
        ,
        …
        ,
        n
        )
      
    
    {\displaystyle y=(y_{i}:i=0,1,2,\ldots ,n)}
  with mean

  
    
      
        
          
            
              μ
              ^
            
          
        
        =
        e
        ⁡
        (
        
          y
          
            i
          
        
        )
        ,
      
    
    {\displaystyle {\widehat {\mu }}=\operatorname {e} (y_{i}),}
  deviations

  
    
      
        
          y
          
            i
          
        
        =
        
          y
          
            i
          
        
        −
        
          
            
              μ
              ^
            
          
        
        ,
      
    
    {\displaystyle y_{i}=y_{i}-{\widehat {\mu }},}
  variance

  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            2
          
        
        =
        e
        ⁡
        (
        
          y
          
            i
          
          
            2
          
        
        )
        ,
      
    
    {\displaystyle {\widehat {\sigma }}^{2}=\operatorname {e} (y_{i}^{2}),}
  and autocorrelation function

  
    
      
        r
        (
        k
        )
        =
        
          
            
              e
              ⁡
              (
              
                y
                
                  i
                
              
              ,
              
                y
                
                  i
                  +
                  k
                
              
              )
            
            
              e
              ⁡
              (
              
                y
                
                  i
                
                
                  2
                
              
              )
            
          
        
      
    
    {\displaystyle r(k)={\frac {\operatorname {e} (y_{i},y_{i+k})}{\operatorname {e} (y_{i}^{2})}}}
  with lag k, if the autocorrelation of this sequence has the long range behavior

  
    
      
        r
        (
        k
        )
        ∼
        
          k
          
            −
            d
          
        
        l
        (
        k
        )
      
    
    {\displaystyle r(k)\sim k^{-d}l(k)}
  as k→∞ and where l(k) is a slowly varying function at large values of k, this sequence is called a self-similar process.the method of expanding bins can be used to analyze self-similar processes.  consider a set of equal-sized non-overlapping bins that divides the original sequence of n elements into groups of m equal-sized segments (n/m is integer) so that new reproductive sequences, based on the mean values, can be defined:

  
    
      
        
          y
          
            i
          
          
            (
            m
            )
          
        
        =
        (
        
          y
          
            i
            m
            −
            m
            +
            1
          
        
        +
        ⋯
        +
        
          y
          
            i
            m
          
        
        )
        
          /
        
        m
        .
      
    
    {\displaystyle y_{i}^{(m)}=(y_{im-m+1}+\cdots +y_{im})/m.}
  the variance determined from this sequence will scale as the bin size changes such that

  
    
      
        var
        ⁡
        [
        
          y
          
            (
            m
            )
          
        
        ]
        =
        
          
            
              
                σ
                ^
              
            
          
          
            2
          
        
        
          m
          
            −
            d
          
        
      
    
    {\displaystyle \operatorname {var} [y^{(m)}]={\widehat {\sigma }}^{2}m^{-d}}
  if and only if the autocorrelation has the limiting form

  
    
      
        
          lim
          
            k
            →
            ∞
          
        
        r
        (
        k
        )
        
          /
        
        
          k
          
            −
            d
          
        
        =
        (
        2
        −
        d
        )
        (
        1
        −
        d
        )
        
          /
        
        2.
      
    
    {\displaystyle \lim _{k\to \infty }r(k)/k^{-d}=(2-d)(1-d)/2.}
  one can also construct a set of corresponding additive sequences

  
    
      
        
          z
          
            i
          
          
            (
            m
            )
          
        
        =
        m
        
          y
          
            i
          
          
            (
            m
            )
          
        
        ,
      
    
    {\displaystyle z_{i}^{(m)}=my_{i}^{(m)},}
  based on the expanding bins,

  
    
      
        
          z
          
            i
          
          
            (
            m
            )
          
        
        =
        (
        
          y
          
            i
            m
            −
            m
            +
            1
          
        
        +
        ⋯
        +
        
          y
          
            i
            m
          
        
        )
        .
      
    
    {\displaystyle z_{i}^{(m)}=(y_{im-m+1}+\cdots +y_{im}).}
  provided the autocorrelation function exhibits the same behavior, the additive sequences will obey the relationship

  
    
      
        var
        ⁡
        [
        
          z
          
            i
          
          
            (
            m
            )
          
        
        ]
        =
        
          m
          
            2
          
        
        var
        ⁡
        [
        
          y
          
            (
            m
            )
          
        
        ]
        =
        
          (
          
            
              
                
                  
                    
                      σ
                      ^
                    
                  
                
                
                  2
                
              
              
                
                  
                    
                      μ
                      ^
                    
                  
                
                
                  2
                  −
                  d
                
              
            
          
          )
        
        e
        ⁡
        [
        
          z
          
            i
          
          
            (
            m
            )
          
        
        
          ]
          
            2
            −
            d
          
        
      
    
    {\displaystyle \operatorname {var} [z_{i}^{(m)}]=m^{2}\operatorname {var} [y^{(m)}]=\left({\frac {{\widehat {\sigma }}^{2}}{{\widehat {\mu }}^{2-d}}}\right)\operatorname {e} [z_{i}^{(m)}]^{2-d}}
  since 
  
    
      
        
          
            
              μ
              ^
            
          
        
      
    
    {\displaystyle {\widehat {\mu }}}
   and 
  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            2
          
        
      
    
    {\displaystyle {\widehat {\sigma }}^{2}}
   are constants this relationship constitutes a variance-to-mean power law, with p = 2 - d.the biconditional relationship above between the variance-to-mean power law and power law autocorrelation function, and the wiener–khinchin theorem imply that any sequence that exhibits a variance-to-mean power law by the method of expanding bins will also manifest 1/f noise, and vice versa.  moreover, the tweedie convergence theorem, by virtue of its central limit-like effect of generating distributions that manifest variance-to-mean power functions, will also generate processes that manifest 1/f noise.  the tweedie convergence theorem thus provides an alternative explanation for the origin of 1/f noise, based its central limit-like effect.
much as the central limit theorem requires certain kinds of random processes to have as a focus of their convergence the gaussian distribution and thus express white noise, the tweedie convergence theorem requires certain non-gaussian processes to have as a focus of convergence the tweedie distributions that express 1/f noise.


=== the tweedie models and multifractality ===
from the properties of self-similar processes, the power-law exponent p = 2 - d is related to the hurst exponent h and the fractal dimension d by

  
    
      
        d
        =
        2
        −
        h
        =
        2
        −
        p
        
          /
        
        2.
      
    
    {\displaystyle d=2-h=2-p/2.}
  a one-dimensional data sequence of self-similar data may demonstrate a variance-to-mean power law with local variations in the value of p and hence in the value of d.  when fractal structures manifest local variations in fractal dimension, they are said to be  multifractals.  examples of data sequences that exhibit local variations in p like this include the eigenvalue deviations of the gaussian orthogonal and unitary ensembles.  the tweedie compound poisson–gamma distribution has served to model multifractality based on local variations in the tweedie exponent α.  consequently, in conjunction with the variation of α, the tweedie convergence theorem can be viewed as having a role in the genesis of such multifractals.
the variation of α has been found to obey the asymmetric laplace distribution in certain cases.  this distribution has been shown to be a member of the family of geometric tweedie models, that manifest as limiting distributions in a convergence theorem for geometric dispersion models.


=== regional organ blood flow ===
regional organ blood flow has been traditionally assessed by the injection of radiolabelled polyethylene microspheres into the arterial circulation of animals, of a size that they become entrapped within the microcirculation of organs.  the organ to be assessed is then divided into equal-sized cubes and the amount of radiolabel within each cube is evaluated by liquid scintillation counting and recorded. the amount of radioactivity within each cube is taken to reflect the blood flow through that sample at the time of injection.  it is possible to evaluate adjacent cubes from an organ in order to additively determine the blood flow through larger regions.  through the work of j b bassingthwaighte and others an empirical power law has been derived between the relative dispersion of blood flow of tissue samples (rd = standard deviation/mean) of mass m relative to reference-sized samples:

  
    
      
        r
        d
        (
        m
        )
        =
        r
        d
        (
        
          m
          
            ref
          
        
        )
        
          
            (
            
              
                m
                
                  m
                  
                    ref
                  
                
              
            
            )
          
          
            1
            −
            
              d
              
                s
              
            
          
        
      
    
    {\displaystyle rd(m)=rd(m_{\text{ref}})\left({\frac {m}{m_{\text{ref}}}}\right)^{1-d_{s}}}
  this power law exponent ds has been called a fractal dimension.  bassingthwaighte's power law can be shown to directly relate to the variance-to-mean power law.  regional organ blood flow can thus be modelled by the tweedie compound poisson–gamma distribution.,  in this model tissue sample could be considered to contain a random (poisson) distributed number of entrapment sites, each with gamma distributed blood flow.  blood flow at this microcirculatory level has been observed to obey a gamma distribution, thus providing support for this hypothesis.


=== cancer metastasis ===
the "experimental cancer metastasis assay"  has some resemblance to the above method to measure regional blood flow.  groups of syngeneic and age matched mice are given intravenous injections of equal-sized aliquots of suspensions of cloned cancer cells and then after a set period of time their lungs are removed and the number of cancer metastases enumerated within each pair of lungs.  if other groups of mice are injected with different cancer cell clones then the number of metastases per group will differ in accordance with the metastatic potentials of the clones.  it has been long recognized that there can be considerable intraclonal variation in the numbers of metastases per mouse despite the best attempts to keep the experimental conditions within each clonal group uniform. this variation is larger than would be expected on the basis of a poisson distribution of numbers of metastases per mouse in each clone and when the variance of the number of metastases per mouse was plotted against the corresponding mean a power law was found.the variance-to-mean power law for metastases was found to also hold for spontaneous murine metastases  and for cases series of human metastases. 
since hematogenous metastasis occurs in direct relationship to regional blood flow and videomicroscopic studies indicate that the passage and entrapment of cancer cells within the circulation appears analogous to the microsphere experiments it seemed plausible to propose that the variation in numbers of hematogenous metastases could reflect heterogeneity in regional organ blood flow.  
the blood flow model was based on the tweedie compound poisson–gamma distribution, a distribution governing a continuous random variable.  for that reason in the metastasis model it was assumed that blood flow was governed by that distribution and that the number of regional metastases occurred as a poisson process for which the intensity was directly proportional to blood flow.  this led to the description of the poisson negative binomial (pnb) distribution as a discrete equivalent to the tweedie compound poisson–gamma distribution.  the probability generating function for the pnb distribution is

  
    
      
        g
        (
        s
        )
        =
        exp
        ⁡
        
          [
          
            λ
            
              
                
                  α
                  −
                  1
                
                α
              
            
            
              
                (
                
                  
                    θ
                    
                      α
                      −
                      1
                    
                  
                
                )
              
              
                α
              
            
            
              {
              
                
                  
                    (
                    
                      1
                      −
                      
                        
                          1
                          θ
                        
                      
                      +
                      
                        
                          s
                          θ
                        
                      
                    
                    )
                  
                  
                    α
                  
                
                −
                1
              
              }
            
          
          ]
        
      
    
    {\displaystyle g(s)=\exp \left[\lambda {\frac {\alpha -1}{\alpha }}\left({\frac {\theta }{\alpha -1}}\right)^{\alpha }\left\{\left(1-{\frac {1}{\theta }}+{\frac {s}{\theta }}\right)^{\alpha }-1\right\}\right]}
  the relationship between the mean and variance of the pnb distribution is then

  
    
      
        var
        ⁡
        (
        y
        )
        =
        a
        e
        ⁡
        (
        y
        
          )
          
            b
          
        
        +
        e
        ⁡
        (
        y
        )
        ,
      
    
    {\displaystyle \operatorname {var} (y)=a\operatorname {e} (y)^{b}+\operatorname {e} (y),}
  which, in the range of many experimental metastasis assays, would be indistinguishable from the variance-to-mean power law.  for sparse data, however, this discrete variance-to-mean relationship would behave more like that of a poisson distribution where the variance equaled the mean.


=== genomic structure and evolution ===
the local density of single nucleotide polymorphisms (snps) within the human genome, as well as that of genes, appears to cluster in accord with the variance-to-mean power law and the tweedie compound poisson–gamma distribution.  in the case of snps their observed density reflects the assessment techniques, the availability of genomic sequences for analysis, and the nucleotide heterozygosity.  the first two factors reflect ascertainment errors inherent to the collection methods, the latter factor reflects an intrinsic property of the genome.
in the coalescent model of population genetics each genetic locus has its own unique history.  within the evolution of a population from some species some genetic loci could presumably be traced back to a relatively recent common ancestor whereas other loci might have more ancient genealogies.  more ancient genomic segments would have had more time to accumulate snps and to experience recombination.  r r hudson has proposed a model where recombination could cause variation in the time to most common recent ancestor for different genomic segments.  a high recombination rate could cause a chromosome to contain a large number of small segments with less correlated genealogies.
assuming a constant background rate of mutation the number of snps per genomic segment would accumulate proportionately to the time to the most recent common ancestor.  current population genetic theory would indicate that these times would be gamma distributed, on average.  the tweedie compound poisson–gamma distribution would suggest a model whereby the snp map would consist of multiple small genomic segments with the mean number of snps per segment would be gamma distributed as per hudson's model.
the distribution of genes within the human genome also demonstrated a variance-to-mean power law, when the method of expanding bins was used to determine the corresponding variances and means. similarly the number of genes per enumerative bin was found to obey a tweedie compound poisson–gamma distribution.  this probability distribution was deemed compatible with two different biological models: the microarrangement model where the number of genes per unit genomic length was determined by the sum of a random number of smaller genomic segments derived by random breakage and reconstruction of protochormosomes.  these smaller segments would be assumed to carry on average a gamma distributed number of genes.
in the alternative gene cluster model, genes would be distributed randomly within the protochromosomes.  over large evolutionary timescales there would occur tandem duplication, mutations, insertions, deletions and rearrangements that could affect the genes through a stochastic birth, death and immigration process to yield the tweedie compound poisson–gamma distribution.
both these mechanisms would implicate neutral evolutionary processes that would result in regional clustering of genes.


=== random matrix theory ===
the gaussian unitary ensemble (gue) consists of complex hermitian matrices that are invariant under unitary transformations whereas the gaussian orthogonal ensemble (goe) consists of real symmetric matrices invariant under orthogonal transformations.  the ranked eigenvalues en from these random matrices obey wigner's semicircular distribution: for a n×n matrix the average density for eigenvalues of size e will be

  
    
      
        
          
            
              ρ
              ¯
            
          
        
        (
        e
        )
        =
        
          
            {
            
              
                
                  
                    
                      2
                      n
                      −
                      
                        e
                        
                          2
                        
                      
                    
                  
                  
                    /
                  
                  π
                
                
                  
                  
                    |
                    e
                    |
                  
                  <
                  
                    
                      2
                      n
                    
                  
                
              
              
                
                  0
                
                
                  
                  
                    |
                    e
                    |
                  
                  >
                  
                    
                      2
                      n
                    
                  
                
              
            
            
          
        
      
    
    {\displaystyle {\bar {\rho }}(e)={\begin{cases}{\sqrt {2n-e^{2}}}/\pi &\quad \left\vert e\right\vert <{\sqrt {2n}}\\0&\quad \left\vert e\right\vert >{\sqrt {2n}}\end{cases}}}
  as e→ ∞ .  integration of the semicircular rule provides the number of eigenvalues on average less than e,

  
    
      
        
          
            
              η
              ¯
            
          
        
        (
        e
        )
        =
        
          
            1
            
              2
              π
            
          
        
        
          [
          
            e
            
              
                2
                n
                −
                
                  e
                  
                    2
                  
                
              
            
            +
            2
            n
            arcsin
            ⁡
            
              (
              
                
                  e
                  
                    2
                    n
                  
                
              
              )
            
            +
            π
            n
          
          ]
        
        .
      
    
    {\displaystyle {\bar {\eta }}(e)={\frac {1}{2\pi }}\left[e{\sqrt {2n-e^{2}}}+2n\arcsin \left({\frac {e}{\sqrt {2n}}}\right)+\pi n\right].}
  the ranked eigenvalues can be unfolded, or renormalized, with the equation

  
    
      
        
          e
          
            n
          
        
        =
        
          
            
              η
              ¯
            
          
        
        (
        e
        )
        =
        
          ∫
          
            −
            ∞
          
          
            
              e
              
                n
              
            
          
        
        
        d
        
          e
          
            ′
          
        
        
          
            
              ρ
              ¯
            
          
        
        (
        
          e
          
            ′
          
        
        )
        .
      
    
    {\displaystyle e_{n}={\bar {\eta }}(e)=\int \limits _{-\infty }^{e_{n}}\,de^{\prime }{\bar {\rho }}(e^{\prime }).}
  this removes the trend of the sequence from the fluctuating portion.  if we look at the absolute value of the difference between the actual and expected cumulative number of eigenvalues

  
    
      
        
          |
          
            
              
                
                  d
                  ¯
                
              
            
            
              n
            
          
          |
        
        =
        
          |
          
            n
            −
            
              
                
                  η
                  ¯
                
              
            
            (
            
              e
              
                n
              
            
            )
          
          |
        
      
    
    {\displaystyle \left|{\bar {d}}_{n}\right|=\left|n-{\bar {\eta }}(e_{n})\right|}
  we obtain a sequence of eigenvalue fluctuations which, using the method of expanding bins, reveals a variance-to-mean power law.
the eigenvalue fluctuations of both the gue and the goe manifest this power law with the power law exponents ranging between 1 and 2, and they similarly manifest 1/f noise spectra.  these eigenvalue fluctuations also correspond to the tweedie compound poisson–gamma distribution and they exhibit multifractality.


=== the distribution of prime numbers ===
the second chebyshev function ψ(x) is given by,

  
    
      
        ψ
        (
        x
        )
        =
        
          ∑
          
            
              
                
                  
                    
                      p
                      
                    
                    ^
                  
                
              
              
                k
              
            
            ≤
            x
          
        
        log
        ⁡
        
          
            
              
                p
                
              
              ^
            
          
        
        =
        
          ∑
          
            n
            ≤
            x
          
        
        λ
        (
        n
        )
      
    
    {\displaystyle \psi (x)=\sum _{{\widehat {p\,}}^{k}\leq x}\log {\widehat {p\,}}=\sum _{n\leq x}\lambda (n)}
  where the summation extends over all prime powers 
  
    
      
        
          
            
              
                
                  p
                  
                
                ^
              
            
          
          
            k
          
        
      
    
    {\displaystyle {\widehat {p\,}}^{k}}
   not exceeding x, x runs over the positive real numbers, and 
  
    
      
        λ
        (
        n
        )
      
    
    {\displaystyle \lambda (n)}
   is the von mangoldt function. the function ψ(x) is related to the prime-counting function π(x), and as such provides information with regards to the distribution of prime numbers amongst the real numbers.  it is asymptotic to x, a statement equivalent to the prime number theorem and it can also be shown to be related to the zeros of the riemann zeta function located on the critical strip ρ, where the real part of the zeta zero ρ is between 0 and 1.  then ψ expressed for x greater than one can be written:

  
    
      
        
          ψ
          
            0
          
        
        (
        x
        )
        =
        x
        −
        
          ∑
          
            ρ
          
        
        
          
            
              x
              
                ρ
              
            
            ρ
          
        
        −
        ln
        ⁡
        2
        π
        −
        
          
            1
            2
          
        
        ln
        ⁡
        (
        1
        −
        
          x
          
            −
            2
          
        
        )
      
    
    {\displaystyle \psi _{0}(x)=x-\sum _{\rho }{\frac {x^{\rho }}{\rho }}-\ln 2\pi -{\frac {1}{2}}\ln(1-x^{-2})}
  where

  
    
      
        
          ψ
          
            0
          
        
        (
        x
        )
        =
        
          lim
          
            ε
            →
            0
          
        
        
          
            
              ψ
              (
              x
              −
              ε
              )
              +
              ψ
              (
              x
              +
              ε
              )
            
            2
          
        
        .
      
    
    {\displaystyle \psi _{0}(x)=\lim _{\varepsilon \rightarrow 0}{\frac {\psi (x-\varepsilon )+\psi (x+\varepsilon )}{2}}.}
  the riemann hypothesis states that the nontrivial zeros of the riemann zeta function all have real part ½.  these zeta function zeros are related to the distribution of prime numbers.  schoenfeld  has shown that if the riemann hypothesis is true then

  
    
      
        δ
        (
        x
        )
        =
        
          |
          
            ψ
            (
            x
            )
            −
            x
          
          |
        
        <
        
          
            x
          
        
        
          log
          
            2
          
        
        ⁡
        (
        x
        )
        
          /
        
        (
        8
        π
        )
      
    
    {\displaystyle \delta (x)=\left\vert \psi (x)-x\right\vert <{\sqrt {x}}\log ^{2}(x)/(8\pi )}
  for all 
  
    
      
        x
        >
        73.2
      
    
    {\displaystyle x>73.2}
  . if we analyze the chebyshev deviations δ(n) on the integers n using the method of expanding bins and plot the variance versus the mean a variance to mean power law can be demonstrated. moreover, these deviations correspond to the tweedie compound poisson-gamma distribution and they exhibit 1/f noise.


=== other applications ===
applications of tweedie distributions include:

actuarial studies
assay analysis 
survival analysis
ecology 
analysis of alcohol consumption in british teenagers 
medical applications 
health economics 
meteorology and climatology 
fisheries 
mertens function
self-organized criticality


== references ==


== further reading ==
dunn, p.k.; smyth, g.k. (2018). generalized linear models with examples in r. new york: springer. doi:10.1007/978-1-4419-0118-7. isbn 978-1-4419-0118-7. chapter 12 is about tweedie distributions and models.
kaas, r. (2005). "compound poisson distribution and glm’s – tweedie’s distribution". in proceedings of the contact forum "3rd actuarial and financial mathematics day", pages 3–12. brussels: royal flemish academy of belgium for science and the arts.
tweedie, m.c.k. (1956). "some statistical properties of inverse gaussian distributions". virginia j. sci. new series. 7: 160–165.